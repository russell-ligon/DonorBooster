{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projectover(posted, completed,expiration):\n",
    "    formatuse = '%Y-%m-%d %H:%M:%S' # The format: see down this page:https://docs.python.org/3/library/datetime.html\n",
    "    otherformat = '%Y-%m-%d'\n",
    "    \n",
    "    #failed projects were never completed, so in those cases, use the expiration date\n",
    "    # if variable is None:\n",
    "    if completed is None:\n",
    "        try:\n",
    "            clock = datetime.datetime.strptime(expiration,formatuse) \n",
    "        except:\n",
    "            try:\n",
    "                clock = datetime.datetime.strptime(expiration,otherformat)\n",
    "            except:\n",
    "                clock = datetime.datetime.strptime('1900-01-01',otherformat)\n",
    "    else:\n",
    "        try:\n",
    "            clock = datetime.datetime.strptime(completed,formatuse) \n",
    "        except:\n",
    "            try:\n",
    "                clock = datetime.datetime.strptime(completed,otherformat) \n",
    "            except:\n",
    "                clock = datetime.datetime.strptime('1900-01-01',otherformat)\n",
    "                \n",
    "    clock=clock.date()\n",
    "    return(clock)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:03:55.740214Z",
     "start_time": "2019-10-01T17:03:49.337675Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-e650902024d2>:11: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option(\"display.max_colwidth\", -1)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/russell/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/russell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Core\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "import nltk, spacy, string\n",
    "from pprint import pprint\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "\n",
    "## NLTK \n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "Pstemmer = nltk.stem.PorterStemmer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import lemmatize\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta, date #for time duration calculations\n",
    "from dateutil.parser import parse #for fuzzy finding year\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "True\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n"
     ]
    }
   ],
   "source": [
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy.sql import table, column, select, update, insert\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#In Python: Define your username and password used above. I've defined the database name (we're \n",
    "#using a dataset on births, so I call it birth_db). \n",
    "dbname = 'donors_db'\n",
    "username = 'russell'\n",
    "pswd = 'bradypodion'\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print(engine.url)\n",
    "# Replace localhost with IP address if accessing a remote server\n",
    "\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "print(engine.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\nSELECT * FROM scraped_project_metrics where state=AL;\n': column \"al\" does not exist\nLINE 2: SELECT * FROM scraped_project_metrics where state=AL;\n                                                          ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUndefinedColumn\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m             \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1587\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcur\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUndefinedColumn\u001b[0m: column \"al\" does not exist\nLINE 2: SELECT * FROM scraped_project_metrics where state=AL;\n                                                          ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-824651212413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgood_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_sql_query\u001b[0;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m    324\u001b[0m     \"\"\"\n\u001b[1;32m    325\u001b[0m     \u001b[0mpandas_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandasSQL_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     return pandas_sql.read_query(\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mread_query\u001b[0;34m(self, sql, index_col, coerce_float, params, parse_dates, chunksize)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_desc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/insight/lib/python3.8/site-packages/pandas/io/sql.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m             \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatabaseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Execution failed on sql '{args[0]}': {exc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDatabaseError\u001b[0m: Execution failed on sql '\nSELECT * FROM scraped_project_metrics where state=AL;\n': column \"al\" does not exist\nLINE 2: SELECT * FROM scraped_project_metrics where state=AL;\n                                                          ^\n"
     ]
    }
   ],
   "source": [
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "\n",
    "### query: from historical data\n",
    "clean_query = \"\"\"\n",
    "SELECT * FROM scraped_project_metrics where state=AL;\n",
    "\"\"\"\n",
    "\n",
    "good_data = pd.read_sql_query(clean_query,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:03:58.740124Z",
     "start_time": "2019-10-01T17:03:55.743629Z"
    }
   },
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data=good_data.dropna(subset=['text'])\n",
    "print(good_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j5=[]\n",
    "for x in good_data['school_metro']:\n",
    "    if x is None:\n",
    "        y = 'none'\n",
    "    else:\n",
    "        y = x\n",
    "        \n",
    "    j5.append(y)\n",
    "\n",
    "\n",
    "good_data['school_metro']=j5\n",
    "\n",
    "k5=[]\n",
    "for x in good_data['resource_type']:\n",
    "    if x is None:\n",
    "        y = 'none'\n",
    "    else:\n",
    "        y = x\n",
    "        \n",
    "    k5.append(y)\n",
    "\n",
    "\n",
    "good_data['resource_type']=k5\n",
    "\n",
    "r5=[]\n",
    "for x in good_data['numStudents']:\n",
    "    if x is None:\n",
    "        y = 0\n",
    "    elif x=='null':\n",
    "        y = 0\n",
    "    else:\n",
    "        y = int(x)\n",
    "        \n",
    "    r5.append(y)\n",
    "\n",
    "\n",
    "good_data['numStudents']=r5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['school_metro'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convert_to_clock_x(m):\n",
    "    m=int(m)\n",
    "    if m == 1:\n",
    "        a = 1\n",
    "    if m == 2:\n",
    "        a = 2\n",
    "    if m == 3:\n",
    "        a = 3\n",
    "    if m == 4:\n",
    "        a = 2\n",
    "    if m == 5:\n",
    "        a = 1\n",
    "    if m == 6:\n",
    "        a = 0\n",
    "    if m == 7:\n",
    "        a = -1\n",
    "    if m == 8:\n",
    "        a = -2\n",
    "    if m == 9:\n",
    "        a = -3\n",
    "    if m == 10:\n",
    "        a = -2\n",
    "    if m == 11:\n",
    "        a = -1\n",
    "    if m == 12:\n",
    "        a = 0\n",
    "    return(a)\n",
    "\n",
    "def Convert_to_clock_y(m):\n",
    "    m=int(m)\n",
    "    if m == 1:\n",
    "        a = 2\n",
    "    if m == 2:\n",
    "        a = 1\n",
    "    if m == 3:\n",
    "        a = 0\n",
    "    if m == 4:\n",
    "        a = -1\n",
    "    if m == 5:\n",
    "        a = -2\n",
    "    if m == 6:\n",
    "        a = -3\n",
    "    if m == 7:\n",
    "        a = -2\n",
    "    if m == 8:\n",
    "        a = -1\n",
    "    if m == 9:\n",
    "        a = 0\n",
    "    if m == 10:\n",
    "        a = 1\n",
    "    if m == 11:\n",
    "        a = 2\n",
    "    if m == 12:\n",
    "        a = 3\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:04:02.309147Z",
     "start_time": "2019-10-01T17:04:02.252247Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_data['cal_month'].value_counts()\n",
    "\n",
    "good_data['pMonth_x'] = good_data.apply(lambda row: Convert_to_clock_x(row.posting_month),axis=1)\n",
    "good_data['pMonth_y'] = good_data.apply(lambda row: Convert_to_clock_y(row.posting_month),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOOL to determine active projects at the time a given project was posted.\n",
    "#if all already in df.Timestamp format ELSE pd.Timestamp(x)\n",
    "\n",
    "#dp = hist_state.date_posted\n",
    "#dc = hist_state.projectover\n",
    "#hist_state['nactive'] = dp.apply(lambda x: ((x>dp) & (x<=dc)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['posting_day']=good_data['posting_day'].str.split(\" \").str[0]\n",
    "good_data['posting_day']=pd.to_numeric(good_data['posting_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['posting_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['postingstring']=good_data['posting_year'].astype(str)+'_'+good_data['posting_month'].astype(str)+'_'+good_data['posting_day'].astype(str)\n",
    "good_data['projectover'] = good_data.apply(lambda row: projectover(row['date_posted'],row['date_completed'],row['date_expiration']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace date time with date object\n",
    "#good_data['projectover'] = good_data.apply(lambda row:row['date_posted'].date(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = good_data.sort_values(by=['posting_year','posting_month','posting_day'], ascending=True)\n",
    "sorted_df['date_posted']=pd.to_datetime(sorted_df['date_posted'])\n",
    "\n",
    "#replace date time with date object\n",
    "sorted_df['date_posted'] = sorted_df.apply(lambda row: datetime.date(row['date_posted'].year, row['date_posted'].month, row['date_posted'].day),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_posting_days = sorted_df.postingstring.unique()\n",
    "unique_posting_dts = sorted_df.date_posted.unique()\n",
    "print(len(unique_posting_dts))\n",
    "print(unique_posting_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = sorted_df.projectover\n",
    "dp = sorted_df.date_posted\n",
    "active_projects=[]\n",
    "\n",
    "for upd in tqdm(unique_posting_dts):\n",
    "    active_projects.append(((upd>dp) & (upd<=dc)).sum())\n",
    "    #dp.apply(lambda x: ((x>dp) & (x<=dc)).sum())\n",
    "\n",
    "#dp = hist_state.date_posted\n",
    "#dc = hist_state.projectover\n",
    "#hist_state['nactive'] = dp.apply(lambda x: ((x>dp) & (x<=dc)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_dict = dict(zip(unique_posting_dts, active_projects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['n_active_at_posting']=sorted_df['date_posted'].map(matched_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['totalPrice'] = sorted_df['totalPrice'].astype(float)\n",
    "sorted_df['totalPrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postedDATE = sorted_df.date_posted\n",
    "hold = sorted_df.drop(['date_posted'], axis=1)\n",
    "#recombine with date_posted as last column\n",
    "sorted_df = pd.concat([hold, postedDATE], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdon=sorted_df #temp holding frame\n",
    "holdon.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_df = holdon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rows for missing dates\n",
    "r = pd.date_range(start=sorted_df.date_posted.min(), end=sorted_df.date_posted.max())\n",
    "r2=pd.DataFrame(r)\n",
    "r2=r2.rename(columns={0:'fulltime'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filler = pd.Series(['NaN']*101,index=sorted_df.columns)\n",
    "\n",
    "#\n",
    "listforadding=[]\n",
    "for checkdate in tqdm(r2['fulltime']):\n",
    "    if (checkdate not in sorted_df.date_posted.values):\n",
    "        filler['date_posted']=checkdate\n",
    "        rowtoadd = pd.DataFrame(filler).transpose()\n",
    "        listforadding.append(rowtoadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsforadding = pd.concat(listforadding)\n",
    "rowsforadding.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    sorted_df[[\"primary_focus_area\", \"n_active_at_posting\",\"cal_month\"]],\n",
    "    columns=[\"cal_month\"],\n",
    "    aggfunc=np.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df = sorted_df.append(pd.DataFrame(data = rowsforadding), ignore_index=True)\n",
    "# sorted_df = sorted_df.sort_values(by=['date_posted'], ascending=True)\n",
    "# print(sorted_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "plt.scatter(sorted_df.n_active_at_posting,sorted_df.days_to_funding, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "sns.stripplot(sorted_df.pMonth_x,sorted_df.pMonth_y, s=30,alpha=0.0021,jitter=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augustfirst='-08-01'\n",
    "augs=[]\n",
    "for y in range(2003,2017,1):\n",
    "    scstrt = datetime.datetime.strptime(str(y)+augustfirst,'%Y-%m-%d')\n",
    "    augs.append(scstrt)\n",
    "augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['posting_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "# create figure and axis objects with subplots()\n",
    "fig,ax = plt.subplots()\n",
    "# make a plot\n",
    "ax.plot(sorted_df.date_posted, sorted_df[\"days_to_funding\"],alpha=0.021,color=\"blue\",marker=\"o\")\n",
    "#set x-axis limits\n",
    "ax.set_xlim(r[0],r[-1])\n",
    "\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"year\",fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"Days to funding\",color=\"blue\",fontsize=14)\n",
    "\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(sorted_df.date_posted, sorted_df.n_active_at_posting,alpha=0.81, color=\"red\")\n",
    "ax2.set_ylabel(\"Number active projects\",color=\"red\",fontsize=14)\n",
    "\n",
    "for p in augs:\n",
    "    plt.axvline(p,  label='pyplot vertical line',color='black')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "# save the plot as a file\n",
    "fig.savefig('ActiveProjects & Time-to-fund over time.png',\n",
    "            format='png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "#plt.scatter(sorted_df.date_posted, sorted_df.n_active_at_posting,alpha=0.021);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean up poverty level\n",
    "sorted_df[\"poverty_clean\"] = sorted_df[\"poverty_level_y\"].str.replace(\" poverty\", \"\")\n",
    "sorted_df['poverty_clean'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up focus level\n",
    "sorted_df[\"primary_focus_subject\"] = sorted_df[\"primary_focus_subject\"].str.replace(\" \", \"_\")\n",
    "sorted_df['primary_focus_subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't start over\n",
    "# pickle_out = open('/home/russell/Documents/DataScience/DonorsChoose/Data/sorted_df.pickle',\"wb\")\n",
    "# pickle.dump(sorted_df, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1HOT encode relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "print(features.shape)\n",
    "features = features.dropna()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resp = composite['days_to_funding']\n",
    "\n",
    "sns.set_context(\"poster\", font_scale=1.3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.distplot(resp.dropna())\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqrt_resp = resp**(.5)\n",
    "sns.distplot(sqrt_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recip = 1/resp\n",
    "sns.distplot(recip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_resp = np.log(resp)\n",
    "sns.distplot(log_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "bxp_resp, fitted_lambda = stats.boxcox(resp)\n",
    "print(fitted_lambda)\n",
    "sns.distplot(bxp_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The baseline predictions are the historical averages\n",
    "baseline_preds = x_test[:, feature_list.index('average')]\n",
    "\n",
    "#Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - y_test)\n",
    "print('Average baseline error: ', round(np.mean(baseline_errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (with states) BOXCOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, fitted_lambda = stats.boxcox(composite.days_to_funding)\n",
    "\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "invbx= inv_boxcox(np.mean(errors),0.1795925846427111)\n",
    "print('Mean Absolute Error:', round(invbx, 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "inv_boxcox(rmse,0.1795925846427111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "i_y_pred=inv_boxcox(y_pred,0.1795925846427111)\n",
    "i_y_test=inv_boxcox(y_test,0.1795925846427111)\n",
    "\n",
    "plt.scatter(i_y_pred,i_y_test, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (withOUT states) BOXCOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting',\n",
    "                  'num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "print(features.shape)\n",
    "features = features.dropna()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "composite.columns\n",
    "#composite=composite[composite.days_to_funding > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = composite.days_to_funding\n",
    "y, fitted_lambda = stats.boxcox(composite.days_to_funding)\n",
    "\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf2 = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf2.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf2.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "invbx= inv_boxcox(np.mean(errors),0.1795925846427111)\n",
    "print('Mean Absolute Error:', round(invbx, 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "inv_boxcox(rmse,0.1795925846427111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf2.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf2\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "i_y_pred=inv_boxcox(y_pred,0.1795925846427111)\n",
    "i_y_test=inv_boxcox(y_test,0.1795925846427111)\n",
    "plt.xlim(0, 140)\n",
    "\n",
    "plt.scatter(i_y_pred,i_y_test, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model BOXCOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "\n",
    "\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n",
    "composite = composite.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = composite.days_to_funding\n",
    "y, fitted_lambda = stats.boxcox(composite.days_to_funding)\n",
    "\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    reg_lambda=1,\n",
    "    gamma=0,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(regressor.feature_importances_.reshape(1, -1), columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "inv_boxcox(rmse,0.1795925846427111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************* *********** ********  ***************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (with states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "plt.scatter(y_pred,y_test, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (withOUT states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y',\n",
    "                  'num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "print(features.shape)\n",
    "features = features.dropna()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf2 = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf2.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf2.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf2.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf2\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "\n",
    "\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n",
    "composite = composite.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    reg_lambda=1,\n",
    "    gamma=0,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(regressor.feature_importances_.reshape(1, -1), columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "\n",
    "\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n",
    "composite = composite.dropna()\n",
    "\n",
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)\n",
    "\n",
    "# Split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf3a = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf3a_random = RandomizedSearchCV(estimator = rf3a, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf3a_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf3b = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = sorted_df[sorted_df.days_to_funding > 0]\n",
    "\n",
    "\n",
    "trimmed=holder[['numStudents','totalPrice',\n",
    "                  'n_active_at_posting',\n",
    "                  'num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(holder[['school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean']])\n",
    "\n",
    "threeweeks=1*(holder['days_to_funding']<22)\n",
    "\n",
    "good_month=np.where((holder['cal_month']=='b_feb')|(holder['cal_month']=='c_mar')|(holder['cal_month']=='h_aug')|(holder['cal_month']=='j_oct'),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['good_month']=good_month\n",
    "trimmed['threeweeks']=threeweeks\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "composite = composite.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite['threeweeks'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.threeweeks\n",
    "#\n",
    "x = composite.drop(['threeweeks'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "logistic_regression = LogisticRegression()\n",
    "model = logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_regression.predict(x_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "accuracy_percentage = 100 * accuracy\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = model.score(x_test, y_test)\n",
    "print(score)\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'magma');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib\n",
    "\n",
    "# font = {'family' : 'normal',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 30}\n",
    "\n",
    "# matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams.update({'font.size': 42})\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, logistic_regression.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic_regression.predict_proba(x_test)[:,1])\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc,linewidth=4)\n",
    "plt.plot([0, 1], [0, 1],'r--',linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.columns)\n",
    "print(logistic_regression.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=logistic_regression\n",
    "\n",
    "feature_importance = abs(clf.coef_[0])\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "sorted_idx\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos[-10:], feature_importance[sorted_idx][-10:], align='center')\n",
    "featax.set_yticks(pos[-10:])\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx][-10:], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "importance = logistic_regression.coef_[0]\n",
    "nombres=x.columns\n",
    "nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = sorted_df[sorted_df.days_to_funding > 0]\n",
    "\n",
    "\n",
    "trimmed=holder[['n_active_at_posting','totalPrice']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(holder[['school_metro',\n",
    "                  'primary_focus_subject','resource_type']])\n",
    "\n",
    "threeweeks=1*(holder['days_to_funding']<22)\n",
    "\n",
    "good_month=np.where((holder['cal_month']=='b_feb')|(holder['cal_month']=='c_mar')|(holder['cal_month']=='h_aug')|(holder['cal_month']=='j_oct'),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['good_month']=good_month\n",
    "trimmed['threeweeks']=threeweeks\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "composite = composite.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite=composite[['threeweeks',\n",
    "                     'n_active_at_posting', \n",
    "                     'totalPrice', \n",
    "                     'good_month',\n",
    "                     'school_metro_urban',\n",
    "                     'primary_focus_subject_Health_&_Wellness',\n",
    "                     'primary_focus_subject_Nutrition',\n",
    "                     'resource_type_Trips',\n",
    "                     'resource_type_Technology',\n",
    "                     'resource_type_Books']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "composite['threeweeks'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.threeweeks\n",
    "#\n",
    "x = composite.drop(['threeweeks'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "\n",
    "x_scaled = scaler.transform(x)\n",
    "#x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "logistic_regression = LogisticRegression()\n",
    "model = logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_regression.predict(x_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "accuracy_percentage = 100 * accuracy\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = model.score(x_test, y_test)\n",
    "print(score)\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'magma');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib\n",
    "\n",
    "# font = {'family' : 'normal',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 30}\n",
    "\n",
    "# matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams.update({'font.size': 42})\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, logistic_regression.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic_regression.predict_proba(x_test)[:,1])\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc,linewidth=4)\n",
    "plt.plot([0, 1], [0, 1],'r--',linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=logistic_regression\n",
    "\n",
    "feature_importance = abs(clf.coef_[0])\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "sorted_idx\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos[-10:], feature_importance[sorted_idx][-10:], align='center')\n",
    "featax.set_yticks(pos[-10:])\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx][-10:], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting whether a project will be funded within 3 weeks\n",
    "\n",
    "\n",
    "\n",
    "now_active = 5000 # how many projects are active *RIGHT* now\n",
    "totalPrice = 50\n",
    "goodmonth = 1\n",
    "school_metro_urban = 1\n",
    "primary_focus_HW = 0\n",
    "primary_focus_Nut = 0\n",
    "resource_type_Trip = 0\n",
    "resource_type_Tech = 0\n",
    "resource_type_Book =1\n",
    "\n",
    "valuearray=np.array([[now_active,totalPrice,goodmonth,school_metro_urban,primary_focus_HW,\n",
    "            primary_focus_Nut,resource_type_Trip,resource_type_Tech,resource_type_Book]])\n",
    " \n",
    "valuearray=scaler.transform(valuearray)\n",
    "print(valuearray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FundedFast = logistic_regression.predict_proba((valuearray.reshape(1, -1)))\n",
    "print(\"Likelihood of getting funded with 3 weeks of posting = \"+str(FundedFast[0][1])) # Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# SAVE MODEL TO DISK\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"simplified_logistic_regression_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(logistic_regression, file)\n",
    "    \n",
    "#SAVE scaler too\n",
    "pkl2_filename = \"simplified_logistic_regression_scaler.pkl\"\n",
    "with open(pkl2_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(iris.data, iris.target)\n",
    "# Extract single tree\n",
    "estimator = model.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = iris.feature_names,\n",
    "                class_names = iris.target_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['formattedstart']=pd.to_datetime(sorted_df['date_posted'])\n",
    "\n",
    "GB=sorted_df.groupby([(sorted_df['formattedstart'].dt.year.values),(sorted_df['formattedstart'].dt.month.values)]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB.plot('n_active_at_posting','days_to_funding',kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:04:58.315745Z",
     "start_time": "2019-10-01T17:04:58.288709Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    good_data[[\"primary_focus_area\", \"days_to_funding\",\"cal_month\"]],\n",
    "    columns=[\"primary_focus_area\"],\n",
    "    aggfunc=np.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_dist(df_to_use, cat_to_subset, var_dist, figw,figh,linew):\n",
    "    plt.figure(figsize=(figw,figh))\n",
    "    sns.set_context( rc={\"lines.linewidth\": linew})\n",
    "    \n",
    "    for grp in sorted(df_to_use[cat_to_subset].unique()):\n",
    "        grp_df = df_to_use.loc[df_to_use[cat_to_subset] == grp]\n",
    "        \n",
    "        sns.distplot(grp_df[var_dist], hist=False, label=grp)\n",
    "        plt.xlim(0, 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['school_metro'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"primary_focus_area\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"cal_month\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"school_metro\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"resource_type\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"poverty_clean\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"grade_level_y\", \"days_to_funding\",12,10,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"posting_year\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T19:33:10.552639Z",
     "start_time": "2019-09-17T19:33:10.546964Z"
    }
   },
   "source": [
    "### Prepare text for use in topic modeling: tokenize, lemmatize, stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:55:08.326732Z",
     "start_time": "2019-09-24T21:55:08.119922Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatization(text, postags=[\"ADJ\"]):\n",
    "    texts_out = [\n",
    "        token.lemma_ if token.lemma_ not in [\"-PRON-\"] else \"\"\n",
    "        for token in nlp(text)\n",
    "        if token.pos_ in postags\n",
    "    ]\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# Run in terminal: python3 -m spacy download en\n",
    "nlp = spacy.load(\"en\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:17.569926Z",
     "start_time": "2019-09-24T21:55:09.321070Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data[\"hook_lemm_adj\"] = good_data[\"text\"].apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:17.608553Z",
     "start_time": "2019-09-24T21:58:17.573682Z"
    }
   },
   "outputs": [],
   "source": [
    "swfull = stopwords.words(\"english\")\n",
    "\n",
    "def stop_wrds(text_str):\n",
    "    if len(text_str) != 0:\n",
    "        return [word.lower() for word in text_str if word not in swfull]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:41:34.993625Z",
     "start_time": "2019-09-26T20:41:34.982612Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords.words(\"english\")[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:52.666464Z",
     "start_time": "2019-09-24T21:58:17.613880Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data[\"hook_lemm_st\"] = good_data[\"hook_lemm_adj\"].apply(stop_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:52.684750Z",
     "start_time": "2019-09-24T21:58:52.669119Z"
    }
   },
   "outputs": [],
   "source": [
    "def token_stem(text_str):\n",
    "    if len(text_str) != 0:\n",
    "        list_tok = [word for word in text_str if len(word) > 2]\n",
    "        return [Pstemmer.stem(word) for word in list_tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:52.833582Z",
     "start_time": "2019-09-24T21:58:52.690910Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = good_data.dropna(subset=[\"hook_lemm_st\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:57.910655Z",
     "start_time": "2019-09-24T21:58:52.836026Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data[\"hook_lemm_adj\"] = good_data[\"hook_lemm_st\"].apply(token_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:57.921301Z",
     "start_time": "2019-09-24T21:58:57.915617Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_data[\"hook_lemm_st\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data[\"hook_lemm_adj\"].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['fulfillmentTrailer'][0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:57.940810Z",
     "start_time": "2019-09-24T21:58:57.925789Z"
    }
   },
   "outputs": [],
   "source": [
    "adj_words = good_data[\"hook_lemm_adj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:58:57.952792Z",
     "start_time": "2019-09-24T21:58:57.945393Z"
    }
   },
   "outputs": [],
   "source": [
    "adj_words = adj_words[pd.isnull(adj_words) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T23:37:19.803139Z",
     "start_time": "2019-09-24T23:37:19.796764Z"
    }
   },
   "outputs": [],
   "source": [
    "adj_words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_descr_adj = corpora.Dictionary(adj_words)\n",
    "\n",
    "corpus = [dictionary_descr_adj.doc2bow(line) for line in adj_words]  # convert corpus to BoW format\n",
    "\n",
    "desc_adj_tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_adj_tfidf = desc_adj_tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_adj_tfidf[45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import sys\n",
    "import re\n",
    "import imgkit\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics \n",
    "import string\n",
    "\n",
    "import lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.datasets import load_rossi\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data =good_data.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['days_to_funding'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['time_fund_med'] =1*(good_data['days_to_funding'] < 7)\n",
    "good_data['time_fund_med'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data cleaning / feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:05.547616Z",
     "start_time": "2019-10-15T03:04:05.472392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "good_data['len_text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:06.819576Z",
     "start_time": "2019-10-15T03:04:06.715715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_data['min_desc'] = 0\n",
    "good_data['min_desc'][good_data['len_text'] <126] =1\n",
    "\n",
    "good_data['min_desc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:07.698796Z",
     "start_time": "2019-10-15T03:04:07.665704Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = good_data.dropna(subset=['len_text'])\n",
    "good_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:09.369392Z",
     "start_time": "2019-10-15T03:04:09.349782Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(good_data['primary_focus_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:10.964962Z",
     "start_time": "2019-10-15T03:04:10.957131Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_dummies(dataset, variable_list):\n",
    "    for var in variable_list:\n",
    "        dataset = pd.concat([dataset, pd.get_dummies(dataset[var], prefix=var)], axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:11.710141Z",
     "start_time": "2019-10-15T03:04:11.637647Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = gen_dummies(good_data, [\"primary_focus_area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:12.600507Z",
     "start_time": "2019-10-15T03:04:12.464814Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = gen_dummies(good_data, [\"resource_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(good_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=good_data[['time_fund_med',\n",
    "                   'len_text',\n",
    "                     'num_words',\n",
    "                     'posting_year',\n",
    "                     'posting_month',\n",
    "                     'totalPrice','numStudents',\n",
    "                     'primary_focus_area_Applied Learning',\n",
    "                     'primary_focus_area_Health & Sports',\n",
    "                     'primary_focus_area_History & Civics',\n",
    "                     'primary_focus_area_Literacy & Language',\n",
    "                     'primary_focus_area_Math & Science',\n",
    "                     'primary_focus_area_Music & The Arts',\n",
    "                     'primary_focus_area_Special Needs',\n",
    "                     'resource_type_Books',\n",
    "                     'resource_type_Other',\n",
    "                     'resource_type_Supplies',\n",
    "                     'resource_type_Technology',\n",
    "                     'resource_type_Trips',\n",
    "                     'resource_type_Visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = trimmed.dropna()\n",
    "trimmed = trimmed.reset_index(drop=True)\n",
    "#trimmed = trimmed.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed['posting_year']=pd.to_numeric(trimmed['posting_year'])\n",
    "trimmed['totalPrice']=pd.to_numeric(trimmed['totalPrice'])\n",
    "trimmed = trimmed.astype({\"numStudents\": str})\n",
    "\n",
    "students=[]\n",
    "for ns in trimmed['numStudents']:\n",
    "    if ns is None:\n",
    "        x = 0\n",
    "    else:\n",
    "        try:\n",
    "            x = int(ns)\n",
    "        except:\n",
    "            x = 0\n",
    "    students.append(x)\n",
    "\n",
    "\n",
    "trimmed['numStudents']=pd.DataFrame(students)\n",
    "#trimmed['numStudents'].value_counts()\n",
    "#trimmed['numStudents']=pd.to_str(trimmed['numStudents'])\n",
    "#trimmed['numStudents'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary output = is a project funded or not?\n",
    "y = trimmed.time_fund_med\n",
    "\n",
    "#\n",
    "x = trimmed.drop(['time_fund_med'], axis=1)\n",
    "x_scaled = preprocessing.scale(x)\n",
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.25)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "logistic_regression = LogisticRegression()\n",
    "model = logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from patsy import dmatrices\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed2 = good_data[['days_to_funding','len_text',\n",
    "                     'num_words',\n",
    "                     'posting_year',\n",
    "                     'posting_month',\n",
    "                     'totalPrice','numStudents',\n",
    "                     'primary_focus_area_Applied Learning',\n",
    "                     'primary_focus_area_Health & Sports',\n",
    "                     'primary_focus_area_History & Civics',\n",
    "                     'primary_focus_area_Literacy & Language',\n",
    "                     'primary_focus_area_Math & Science',\n",
    "                     'primary_focus_area_Music & The Arts',\n",
    "                     'primary_focus_area_Special Needs',\n",
    "                     'resource_type_Books',\n",
    "                     'resource_type_Other',\n",
    "                     'resource_type_Supplies',\n",
    "                     'resource_type_Technology',\n",
    "                     'resource_type_Trips',\n",
    "                     'resource_type_Visitors']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed2.columns=[re.sub(' ','_',x) for x in trimmed2.columns]\n",
    "trimmed2.columns=[re.sub('_&_','_',x) for x in trimmed2.columns]\n",
    "trimmed2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training and testing data sets.\n",
    "mask = np.random.rand(len(trimmed2)) < 0.8\n",
    "df_train = trimmed2[mask]\n",
    "df_test = trimmed2[~mask]\n",
    "print('Training data set length='+str(len(df_train)))\n",
    "print('Testing data set length='+str(len(df_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup the regression expression in patsy notation. We are telling patsy that BB_COUNT is our dependent variable and\n",
    "# it depends on the regression variables: DAY, DAY_OF_WEEK, MONTH, HIGH_T, LOW_T and PRECIP.\n",
    "expr = \"\"\"days_to_funding ~ len_text  + num_words + posting_month + totalPrice + primary_focus_area_Applied_Learning + primary_focus_area_Health_Sports + primary_focus_area_History_Civics + primary_focus_area_Literacy_Language + primary_focus_area_Math & Science + primary_focus_area_Music_The_Arts + primary_focus_area_Special_Needs + resource_type_Books + resource_type_Other + resource_type_Supplies + resource_type_Technology + resource_type_Trips + resource_type_Visitors\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up the X and y matrices\n",
    "y_train, X_train = dmatrices(expr, df_train, return_type='dataframe')\n",
    "y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:52:13.661221Z",
     "start_time": "2019-09-18T17:52:13.656866Z"
    }
   },
   "source": [
    "### Data Exploration of Full Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:14.406771Z",
     "start_time": "2019-10-15T03:04:14.392563Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:17.852079Z",
     "start_time": "2019-10-15T03:04:17.049000Z"
    }
   },
   "outputs": [],
   "source": [
    "age_stats = sns.catplot(x=\"age\", kind=\"count\", hue=\"time_adpt_med\", data=dog_data_SF_19)\n",
    "\n",
    "age_stats.set(xlabel=\"Age of Dog\", ylabel=\"Number of Dogs\")\n",
    "age_stats._legend.set_title(\"Adopted within 2 weeks\")\n",
    "\n",
    "plt.savefig(\"Demo_Figures/status_age.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:19.910215Z",
     "start_time": "2019-10-15T03:04:19.895879Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T18:41:33.110591Z",
     "start_time": "2019-10-02T18:41:32.427267Z"
    }
   },
   "outputs": [],
   "source": [
    "status_fig = sns.catplot(\n",
    "    x=\"size\",\n",
    "    kind=\"count\",\n",
    "    hue=\"time_adpt_med\",\n",
    "    data=dog_data_SF_19 )\n",
    "\n",
    "status_fig.set(xlabel='Size of Dog', ylabel=\"Number of Dogs\")\n",
    "status_fig._legend.set_title(\"Adopted within 2 weeks\")\n",
    "\n",
    "plt.savefig('Demo_Figures/status_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:18:48.113536Z",
     "start_time": "2019-10-01T17:18:48.098744Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T18:43:03.670141Z",
     "start_time": "2019-10-02T18:43:02.978991Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status_fig = sns.catplot(\n",
    "    x=\"gender\",\n",
    "    kind=\"count\",\n",
    "    hue=\"time_adpt_med\",\n",
    "    data=dog_data_SF_19)\n",
    "\n",
    "status_fig.set(xlabel='Gender of Dog', ylabel=\"Number of Dogs\")\n",
    "status_fig._legend.set_title(\"Adopted \\n within 2 weeks\")\n",
    "\n",
    "plt.savefig('Demo_Figures/status_gender.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:18:55.218716Z",
     "start_time": "2019-10-01T17:18:54.919213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(dog_data_SF_19['time_in_shelter']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:29.404266Z",
     "start_time": "2019-10-01T03:55:29.001555Z"
    }
   },
   "outputs": [],
   "source": [
    "def comp_dist(df_to_use, cat_to_subset, var_dist):\n",
    "    for grp in df_to_use[cat_to_subset].unique():\n",
    "        grp_df = df_to_use.loc[df_to_use[cat_to_subset] == grp]\n",
    "        sns.distplot(grp_df[var_dist], hist=False, label=grp)\n",
    "        plt.xlim(0, 90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "comp_dist(dog_data_SF_19, \"age\", \"time_in_shelter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:30.509733Z",
     "start_time": "2019-10-01T03:55:30.234338Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'gender','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:31.454646Z",
     "start_time": "2019-10-01T03:55:31.198442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'size','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:32.796671Z",
     "start_time": "2019-10-01T03:55:32.755384Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['coat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:34.481739Z",
     "start_time": "2019-10-01T03:55:34.092245Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'coat','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:35.793939Z",
     "start_time": "2019-10-01T03:55:35.548201Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'time_adpt_med','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:38.141987Z",
     "start_time": "2019-10-01T03:55:38.128767Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['time_adpt_med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:33.694951Z",
     "start_time": "2019-10-15T03:04:33.670072Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    dog_data_SF_19[[\"time_adpt_med\", \"time_in_shelter\"]],\n",
    "    columns=\"time_adpt_med\",\n",
    "    aggfunc=np.min\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:05:05.078834Z",
     "start_time": "2019-10-15T03:05:04.839963Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19[features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:42:29.355887Z",
     "start_time": "2019-10-04T20:42:29.147910Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking out relationships \n",
    "full_data_corr = dog_data_SF_19[features].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:34:47.782486Z",
     "start_time": "2019-10-04T20:34:42.504474Z"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(40, 40))\n",
    "# sns.set(font_scale=1)  \n",
    "sns.heatmap(\n",
    "    full_data_corr,\n",
    "#     annot=True,\n",
    "#     annot_kws={\"size\": 40},\n",
    "    cmap=\"GnBu\",\n",
    "#     cbar=False,\n",
    "    fmt=\" \",\n",
    "    ax=ax,\n",
    ");\n",
    "# ax.set_ylim(2, 0);\n",
    "\n",
    "plt.savefig(\"Demo_Figures/full_corr_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data cleaning / feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:05.547616Z",
     "start_time": "2019-10-15T03:04:05.472392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19['len_descr'][dog_data_SF_19['missing_description']==1] = 0\n",
    "\n",
    "dog_data_SF_19['len_descr'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:06.819576Z",
     "start_time": "2019-10-15T03:04:06.715715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19['min_desc'] = 0\n",
    "dog_data_SF_19['min_desc'][dog_data_SF_19['len_descr'] <307] =1\n",
    "\n",
    "dog_data_SF_19['min_desc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:07.698796Z",
     "start_time": "2019-10-15T03:04:07.665704Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19 = dog_data_SF_19.dropna(subset=['len_descr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:09.369392Z",
     "start_time": "2019-10-15T03:04:09.349782Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['colors_cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:10.964962Z",
     "start_time": "2019-10-15T03:04:10.957131Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_dummies(dataset, variable_list):\n",
    "    for var in variable_list:\n",
    "        dataset = pd.concat([dataset, pd.get_dummies(dataset[var], prefix=var)], axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:11.710141Z",
     "start_time": "2019-10-15T03:04:11.637647Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19 = gen_dummies(dog_data_SF_19, [\"pub_year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:12.600507Z",
     "start_time": "2019-10-15T03:04:12.464814Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19 = gen_dummies(dog_data_SF_19, [\"breeds.primary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T17:52:13.661221Z",
     "start_time": "2019-09-18T17:52:13.656866Z"
    }
   },
   "source": [
    "### Data Exploration of Full Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:14.406771Z",
     "start_time": "2019-10-15T03:04:14.392563Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:17.852079Z",
     "start_time": "2019-10-15T03:04:17.049000Z"
    }
   },
   "outputs": [],
   "source": [
    "age_stats = sns.catplot(x=\"age\", kind=\"count\", hue=\"time_adpt_med\", data=dog_data_SF_19)\n",
    "\n",
    "age_stats.set(xlabel=\"Age of Dog\", ylabel=\"Number of Dogs\")\n",
    "age_stats._legend.set_title(\"Adopted within 2 weeks\")\n",
    "\n",
    "plt.savefig(\"Demo_Figures/status_age.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:19.910215Z",
     "start_time": "2019-10-15T03:04:19.895879Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T18:41:33.110591Z",
     "start_time": "2019-10-02T18:41:32.427267Z"
    }
   },
   "outputs": [],
   "source": [
    "status_fig = sns.catplot(\n",
    "    x=\"size\",\n",
    "    kind=\"count\",\n",
    "    hue=\"time_adpt_med\",\n",
    "    data=dog_data_SF_19 )\n",
    "\n",
    "status_fig.set(xlabel='Size of Dog', ylabel=\"Number of Dogs\")\n",
    "status_fig._legend.set_title(\"Adopted within 2 weeks\")\n",
    "\n",
    "plt.savefig('Demo_Figures/status_size.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:18:48.113536Z",
     "start_time": "2019-10-01T17:18:48.098744Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-02T18:43:03.670141Z",
     "start_time": "2019-10-02T18:43:02.978991Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "status_fig = sns.catplot(\n",
    "    x=\"gender\",\n",
    "    kind=\"count\",\n",
    "    hue=\"time_adpt_med\",\n",
    "    data=dog_data_SF_19)\n",
    "\n",
    "status_fig.set(xlabel='Gender of Dog', ylabel=\"Number of Dogs\")\n",
    "status_fig._legend.set_title(\"Adopted \\n within 2 weeks\")\n",
    "\n",
    "plt.savefig('Demo_Figures/status_gender.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:18:55.218716Z",
     "start_time": "2019-10-01T17:18:54.919213Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(dog_data_SF_19['time_in_shelter']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:29.404266Z",
     "start_time": "2019-10-01T03:55:29.001555Z"
    }
   },
   "outputs": [],
   "source": [
    "def comp_dist(df_to_use, cat_to_subset, var_dist):\n",
    "    for grp in df_to_use[cat_to_subset].unique():\n",
    "        grp_df = df_to_use.loc[df_to_use[cat_to_subset] == grp]\n",
    "        sns.distplot(grp_df[var_dist], hist=False, label=grp)\n",
    "        plt.xlim(0, 90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "comp_dist(dog_data_SF_19, \"age\", \"time_in_shelter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:30.509733Z",
     "start_time": "2019-10-01T03:55:30.234338Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'gender','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:31.454646Z",
     "start_time": "2019-10-01T03:55:31.198442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'size','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:32.796671Z",
     "start_time": "2019-10-01T03:55:32.755384Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['coat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:34.481739Z",
     "start_time": "2019-10-01T03:55:34.092245Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'coat','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:35.793939Z",
     "start_time": "2019-10-01T03:55:35.548201Z"
    }
   },
   "outputs": [],
   "source": [
    "comp_dist(dog_data_SF_19,'time_adpt_med','time_in_shelter')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T03:55:38.141987Z",
     "start_time": "2019-10-01T03:55:38.128767Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(dog_data_SF_19['time_adpt_med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:33.694951Z",
     "start_time": "2019-10-15T03:04:33.670072Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    dog_data_SF_19[[\"time_adpt_med\", \"time_in_shelter\"]],\n",
    "    columns=\"time_adpt_med\",\n",
    "    aggfunc=np.min\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:05:05.078834Z",
     "start_time": "2019-10-15T03:05:04.839963Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_19[features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:42:29.355887Z",
     "start_time": "2019-10-04T20:42:29.147910Z"
    }
   },
   "outputs": [],
   "source": [
    "#Checking out relationships \n",
    "full_data_corr = dog_data_SF_19[features].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-04T20:34:47.782486Z",
     "start_time": "2019-10-04T20:34:42.504474Z"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(40, 40))\n",
    "# sns.set(font_scale=1)  \n",
    "sns.heatmap(\n",
    "    full_data_corr,\n",
    "#     annot=True,\n",
    "#     annot_kws={\"size\": 40},\n",
    "    cmap=\"GnBu\",\n",
    "#     cbar=False,\n",
    "    fmt=\" \",\n",
    "    ax=ax,\n",
    ");\n",
    "# ax.set_ylim(2, 0);\n",
    "\n",
    "plt.savefig(\"Demo_Figures/full_corr_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Additional data cleaning / feature extraction\n",
    "\n",
    "dog_data_SF_19['len_descr'][dog_data_SF_19['missing_description']==1] = 0\n",
    "\n",
    "dog_data_SF_19['len_descr'].describe()\n",
    "\n",
    "dog_data_SF_19['min_desc'] = 0\n",
    "dog_data_SF_19['min_desc'][dog_data_SF_19['len_descr'] <307] =1\n",
    "\n",
    "dog_data_SF_19['min_desc'].describe()\n",
    "\n",
    "dog_data_SF_19 = dog_data_SF_19.dropna(subset=['len_descr'])\n",
    "\n",
    "pd.Categorical.describe(dog_data_SF_19['colors_cat'])\n",
    "\n",
    "def gen_dummies(dataset, variable_list):\n",
    "    for var in variable_list:\n",
    "        dataset = pd.concat([dataset, pd.get_dummies(dataset[var], prefix=var)], axis=1)\n",
    "    return dataset\n",
    "\n",
    "dog_data_SF_19 = gen_dummies(dog_data_SF_19, [\"pub_year\"])\n",
    "\n",
    "dog_data_SF_19 = gen_dummies(dog_data_SF_19, [\"breeds.primary\"])\n",
    "\n",
    "### Data Exploration of Full Data Set\n",
    "\n",
    "pd.Categorical.describe(dog_data_SF_19['age'])\n",
    "\n",
    "age_stats = sns.catplot(x=\"age\", kind=\"count\", hue=\"time_adpt_med\", data=dog_data_SF_19)\n",
    "\n",
    "age_stats.set(xlabel=\"Age of Dog\", ylabel=\"Number of Dogs\")\n",
    "age_stats._legend.set_title(\"Adopted within 2 weeks\")\n",
    "\n",
    "plt.savefig(\"Demo_Figures/status_age.png\")\n",
    "\n",
    "pd.Categorical.describe(dog_data_SF_19['size'])\n",
    "\n",
    "status_fig = sns.catplot(\n",
    "    x=\"size\",\n",
    "    kind=\"count\",\n",
    "    hue=\"time_adpt_med\",\n",
    "    data=dog_data_SF_19 )\n",
    "\n",
    "status_fig.set(xlabel='Size of Dog', ylabel=\"Number of Dogs\")\n",
    "status_fig._legend.set_title(\"Adopted within 2 weeks\")\n",
    "\n",
    "plt.savefig('Demo_Figures/status_size.png')\n",
    "plt.show()\n",
    "\n",
    "pd.Categorical.describe(dog_data_SF_19['gender'])\n",
    "\n",
    "status_fig = sns.catplot(\n",
    "    x=\"gender\",\n",
    "    kind=\"count\",\n",
    "    hue=\"time_adpt_med\",\n",
    "    data=dog_data_SF_19)\n",
    "\n",
    "status_fig.set(xlabel='Gender of Dog', ylabel=\"Number of Dogs\")\n",
    "status_fig._legend.set_title(\"Adopted \\n within 2 weeks\")\n",
    "\n",
    "plt.savefig('Demo_Figures/status_gender.png')\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(dog_data_SF_19['time_in_shelter']);\n",
    "\n",
    "def comp_dist(df_to_use, cat_to_subset, var_dist):\n",
    "    for grp in df_to_use[cat_to_subset].unique():\n",
    "        grp_df = df_to_use.loc[df_to_use[cat_to_subset] == grp]\n",
    "        sns.distplot(grp_df[var_dist], hist=False, label=grp)\n",
    "        plt.xlim(0, 90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "comp_dist(dog_data_SF_19, \"age\", \"time_in_shelter\")\n",
    "\n",
    "comp_dist(dog_data_SF_19,'gender','time_in_shelter')        \n",
    "\n",
    "comp_dist(dog_data_SF_19,'size','time_in_shelter')        \n",
    "\n",
    "pd.Categorical.describe(dog_data_SF_19['coat'])\n",
    "\n",
    "comp_dist(dog_data_SF_19,'coat','time_in_shelter')        \n",
    "\n",
    "comp_dist(dog_data_SF_19,'time_adpt_med','time_in_shelter')        \n",
    "\n",
    "pd.Categorical.describe(dog_data_SF_19['time_adpt_med'])\n",
    "\n",
    "pd.pivot_table(\n",
    "    dog_data_SF_19[[\"time_adpt_med\", \"time_in_shelter\"]],\n",
    "    columns=\"time_adpt_med\",\n",
    "    aggfunc=np.min\n",
    ")\n",
    "\n",
    "dog_data_SF_19[features].describe()\n",
    "\n",
    "#Checking out relationships \n",
    "full_data_corr = dog_data_SF_19[features].corr()\n",
    "\n",
    "f, ax = plt.subplots(figsize=(40, 40))\n",
    "# sns.set(font_scale=1)  \n",
    "sns.heatmap(\n",
    "    full_data_corr,\n",
    "#     annot=True,\n",
    "#     annot_kws={\"size\": 40},\n",
    "    cmap=\"GnBu\",\n",
    "#     cbar=False,\n",
    "    fmt=\" \",\n",
    "    ax=ax,\n",
    ");\n",
    "# ax.set_ylim(2, 0);\n",
    "\n",
    "plt.savefig(\"Demo_Figures/full_corr_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-21T23:14:31.314050Z",
     "start_time": "2019-09-21T23:14:31.299197Z"
    }
   },
   "source": [
    "### Model Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T21:12:29.503966Z",
     "start_time": "2019-09-24T21:12:29.498653Z"
    }
   },
   "outputs": [],
   "source": [
    "## This was used to select the best pos subset \n",
    "score_tracking = {}\n",
    "\n",
    "def score_lda(model_name, lda_model_adj_tfidf, corpus_adj_tfidf, dictionary_descr_adj):\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model_adj_tfidf,corpus=corpus_adj_tfidf,\n",
    "        dictionary=dictionary_descr_adj, coherence=\"u_mass\")\n",
    "\n",
    "        score_tracking[model_name] = coherence_model_lda.get_coherence()\n",
    "        return coherence_model_lda.get_coherence()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:00:45.828013Z",
     "start_time": "2019-09-24T22:00:44.551555Z"
    }
   },
   "outputs": [],
   "source": [
    "score_lda('TFIDF_A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:00:45.846623Z",
     "start_time": "2019-09-24T22:00:45.830727Z"
    }
   },
   "outputs": [],
   "source": [
    "score_tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:16:50.191971Z",
     "start_time": "2019-09-24T22:16:50.180367Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(score_tracking, orient=\"index\").to_csv(\"model_testing_coherence.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:17:15.339097Z",
     "start_time": "2019-09-24T22:17:15.330496Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(\n",
    "            corpus=corpus,\n",
    "            id2word=dictionary,\n",
    "            num_topics=num_topics,\n",
    "            random_state=100,\n",
    "            update_every=1,\n",
    "            chunksize=10,\n",
    "            passes=10,\n",
    "            alpha=\"symmetric\",\n",
    "            iterations=100,\n",
    "            per_word_topics=True,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(\n",
    "            model=model, corpus=corpus, dictionary=dictionary, coherence=\"u_mass\"\n",
    "        )\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:57:10.717586Z",
     "start_time": "2019-09-24T22:17:34.928418Z"
    }
   },
   "outputs": [],
   "source": [
    "#TAKES A REALLY LONG TIME TO RUN ON MY DATASET\n",
    "model_list, coherence_vals = compute_coherence_values(\n",
    "    dictionary_descr_adj, corpus_adj_tfidf, limit=40, step=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:17:15.339097Z",
     "start_time": "2019-09-24T22:17:15.330496Z"
    }
   },
   "source": [
    "print(start)\n",
    "print(limit)\n",
    "print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T22:17:15.339097Z",
     "start_time": "2019-09-24T22:17:15.330496Z"
    }
   },
   "source": [
    "start=2\n",
    "limit=3\n",
    "step=1\n",
    "\n",
    "\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "for num_topics in range(start, limit, step):\n",
    "    print(num_topics)\n",
    "    model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary_descr_adj,\n",
    "        num_topics=num_topics,\n",
    "        random_state=100,\n",
    "        update_every=1,\n",
    "        chunksize=10,\n",
    "        passes=10,\n",
    "        alpha=\"symmetric\",\n",
    "        iterations=100,\n",
    "        per_word_topics=True,\n",
    "    )\n",
    "    model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(\n",
    "        model=model, corpus=corpus, dictionary=dictionary_descr_adj, coherence=\"u_mass\"\n",
    "    )\n",
    "    coherence_values.append(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T23:39:29.016634Z",
     "start_time": "2019-09-24T23:39:28.824181Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit=40; start=2; step=3;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_vals)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score u_mass\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:31:39.452157Z",
     "start_time": "2019-09-25T00:31:39.278385Z"
    }
   },
   "outputs": [],
   "source": [
    "limit=40; start=2; step=3;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_vals)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence c_v score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:31:39.452157Z",
     "start_time": "2019-09-25T00:31:39.278385Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "limit=40; start=2; step=3;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_vals)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence c_v score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-27T21:00:27.853902Z",
     "start_time": "2019-09-27T19:54:27.070339Z"
    }
   },
   "outputs": [],
   "source": [
    "model_list_long, coherence_vals_long = compute_coherence_values(\n",
    "    dictionary_descr_adj, corpus_adj_tfidf, limit=80, step=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T02:58:21.896372Z",
     "start_time": "2019-09-28T02:58:21.888193Z"
    }
   },
   "outputs": [],
   "source": [
    "model_list_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T03:00:19.084903Z",
     "start_time": "2019-09-28T03:00:19.078630Z"
    }
   },
   "outputs": [],
   "source": [
    "coherence_vals_long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T02:57:14.393014Z",
     "start_time": "2019-09-28T02:57:14.137019Z"
    }
   },
   "outputs": [],
   "source": [
    "limit=80; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_vals_long)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence c_v score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T03:01:18.650179Z",
     "start_time": "2019-09-28T03:01:18.640146Z"
    }
   },
   "outputs": [],
   "source": [
    "model_list_long[7].print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:29:38.589051Z",
     "start_time": "2019-09-25T21:29:38.558689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_list[5].print_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:01:56.802017Z",
     "start_time": "2019-09-25T21:01:56.790381Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_details = pd.DataFrame((model_list[5].print_topics(num_words=20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:07:53.243300Z",
     "start_time": "2019-09-25T21:07:53.225802Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:02:22.542889Z",
     "start_time": "2019-09-25T21:02:22.533818Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_details.to_csv(\"/home/russell/Documents/DataScience/DonorsChoose/DonorBooster/topics_files.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T19:48:37.676552Z",
     "start_time": "2019-09-25T19:48:37.665472Z"
    }
   },
   "outputs": [],
   "source": [
    "## pickle up the model \n",
    "\n",
    "filename_mdl = 'lda_model_17_tps.sav'\n",
    "pickle.dump(model_list[5], open(filename_mdl, 'wb'))\n",
    "\n",
    "filename_dct = 'lda_dict_17_tps.sav'\n",
    "pickle.dump(dictionary_descr_adj, open(filename_dct, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:32:25.267671Z",
     "start_time": "2019-09-25T21:30:24.210370Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:32:25.267671Z",
     "start_time": "2019-09-25T21:30:24.210370Z"
    }
   },
   "outputs": [],
   "source": [
    "LDAvis_prepared = pyLDAvis.gensim.prepare(model_list[5], corpus_adj_tfidf, dictionary_descr_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T21:32:25.267671Z",
     "start_time": "2019-09-25T21:30:24.210370Z"
    }
   },
   "outputs": [],
   "source": [
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T20:25:30.009524Z",
     "start_time": "2019-09-25T20:25:30.004696Z"
    }
   },
   "outputs": [],
   "source": [
    "the_model = model_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:07:14.509187Z",
     "start_time": "2019-09-28T04:07:14.500831Z"
    }
   },
   "outputs": [],
   "source": [
    "test_corp = dictionary_descr_adj.doc2bow(\n",
    "    [\n",
    "        \"Outgoing\",\n",
    "        \"Amazing\",\n",
    "        \"energetic\",\n",
    "        \"trainable\",\n",
    "        \"Typical\",\n",
    "        \"puppy\",\n",
    "        \"Confident\",\n",
    "        \"loyal\",\n",
    "        \"Quiet\",\n",
    "        \"delightful\",\n",
    "        \"Wonderful\",\n",
    "        \"affectionate\",\n",
    "        \"nice\",\n",
    "        \"Playful\",\n",
    "        \"Calm\",\n",
    "        \"Active\",\n",
    "        \"Smart\",\n",
    "        \"Strong\",\n",
    "        \"secure\",\n",
    "        \"Ideal\",\n",
    "        \"Easy\",\n",
    "        \"going\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:08:01.614828Z",
     "start_time": "2019-09-28T04:08:01.608703Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(model_list[5].get_document_topics(test_corp, minimum_probability=0.08))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:08:02.292520Z",
     "start_time": "2019-09-28T04:08:02.283669Z"
    }
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T04:08:12.449206Z",
     "start_time": "2019-09-28T04:08:12.443765Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_list[5].get_document_topics(['ugly'], minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict out, merge with all data and export for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:49:11.119472Z",
     "start_time": "2019-09-25T00:49:11.093797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_adj_topics = model_list[5].get_document_topics(corpus_adj_tfidf, minimum_probability=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:51:24.178136Z",
     "start_time": "2019-09-26T20:51:24.172680Z"
    }
   },
   "outputs": [],
   "source": [
    "list_top = pd.DataFrame(lda_adj_topics[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:51:35.329739Z",
     "start_time": "2019-09-26T20:51:35.322740Z"
    }
   },
   "outputs": [],
   "source": [
    "list_top[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:52:14.015925Z",
     "start_time": "2019-09-26T20:52:14.007068Z"
    }
   },
   "outputs": [],
   "source": [
    "len([top for top in list_top if top in [0,2,3,4,6,7,8,9,10,11,12,13,16]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:49:16.065687Z",
     "start_time": "2019-09-25T00:49:16.028804Z"
    }
   },
   "outputs": [],
   "source": [
    "adj_words_df =pd.DataFrame(adj_words)\n",
    "adj_words_df['all_index'] = adj_words_df.index\n",
    "adj_words_df = adj_words_df.reindex(range(0,21938))\n",
    "adj_words_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:49:22.410648Z",
     "start_time": "2019-09-25T00:49:17.315798Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_topics_df  = pd.DataFrame(lda_adj_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:49:24.023250Z",
     "start_time": "2019-09-25T00:49:23.957105Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_topics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:49:33.713397Z",
     "start_time": "2019-09-25T00:49:33.691153Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data_topics_df = pd.concat([adj_words_df, lda_topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:49:35.110644Z",
     "start_time": "2019-09-25T00:49:34.963504Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data_topics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:51:20.250339Z",
     "start_time": "2019-09-25T00:51:20.244169Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_topics_df.columns = [\n",
    "    \"desc_lemm_adj\",\n",
    "    \"all_index\",\n",
    "    \"topic_1\",\n",
    "    \"topic_2\",\n",
    "    \"topic_3\",\n",
    "    \"topic_4\",\n",
    "    \"topic_5\",\n",
    "    \"topic_6\",\n",
    "    \"topic_7\",\n",
    "    \"topic_8\",\n",
    "    \"topic_9\",\n",
    "    \"topic_10\",\n",
    "    \"topic_11\",\n",
    "    \"topic_12\",\n",
    "    \"topic_13\",\n",
    "    \"topic_14\",\n",
    "    \"topic_15\",\n",
    "    \"topic_16\",\n",
    "    \"topic_17\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:51:23.776765Z",
     "start_time": "2019-09-25T00:51:23.525778Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps = dog_data_SF_19_full.merge(\n",
    "    all_data_topics_df,\n",
    "    how=\"inner\",\n",
    "    left_index=True,\n",
    "    right_on='all_index'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T00:51:25.687278Z",
     "start_time": "2019-09-25T00:51:25.442633Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T02:10:30.785127Z",
     "start_time": "2019-09-25T02:10:30.677634Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps = dog_data_SF_w_tps.dropna(subset=[\"topic_1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T02:10:31.640092Z",
     "start_time": "2019-09-25T02:10:31.625101Z"
    }
   },
   "outputs": [],
   "source": [
    "sum(dog_data_SF_w_tps['desc_lemm_adj_x'] == dog_data_SF_w_tps['desc_lemm_adj_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T02:10:35.377082Z",
     "start_time": "2019-09-25T02:10:35.365871Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps['topic_17'][1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:03:45.403255Z",
     "start_time": "2019-09-25T03:03:45.398409Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_topic_val(topic_tpl, thres_topc=0.2):\n",
    "    value = topic_tpl[1]\n",
    "    if value > thres_topc:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:03:47.388905Z",
     "start_time": "2019-09-25T03:03:46.475556Z"
    }
   },
   "outputs": [],
   "source": [
    "for row in [\n",
    "    \"topic_1\",\n",
    "    \"topic_2\",\n",
    "    \"topic_3\",\n",
    "    \"topic_4\",\n",
    "    \"topic_5\",\n",
    "    \"topic_6\",\n",
    "    \"topic_7\",\n",
    "    \"topic_8\",\n",
    "    \"topic_9\",\n",
    "    \"topic_10\",\n",
    "    \"topic_11\",\n",
    "    \"topic_12\",\n",
    "    \"topic_13\",\n",
    "    \"topic_14\",\n",
    "    \"topic_15\",\n",
    "    \"topic_16\",\n",
    "    \"topic_17\"\n",
    "]:\n",
    "    dog_data_SF_w_tps[row + \"_val\"] = dog_data_SF_w_tps[row].apply(convert_topic_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T03:53:40.476147Z",
     "start_time": "2019-09-26T03:53:40.111123Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_val(topic_tpl):\n",
    "    return topic_tpl[1]\n",
    "\n",
    "for row in [\n",
    "    \"topic_1\",\n",
    "    \"topic_2\",\n",
    "    \"topic_3\",\n",
    "    \"topic_4\",\n",
    "    \"topic_5\",\n",
    "    \"topic_6\",\n",
    "    \"topic_7\",\n",
    "    \"topic_8\",\n",
    "    \"topic_9\",\n",
    "    \"topic_10\",\n",
    "    \"topic_11\",\n",
    "    \"topic_12\",\n",
    "    \"topic_13\",\n",
    "    \"topic_14\",\n",
    "    \"topic_15\",\n",
    "    \"topic_16\",\n",
    "    \"topic_17\"\n",
    "]:\n",
    "    dog_data_SF_w_tps[row + \"_val_full\"] = dog_data_SF_w_tps[row].apply(convert_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T04:15:15.520860Z",
     "start_time": "2019-09-26T04:15:15.508434Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps['topic_11_val_full'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T20:43:36.415882Z",
     "start_time": "2019-09-26T20:43:36.413166Z"
    }
   },
   "outputs": [],
   "source": [
    "# dog_data_SF_w_tps['desc_full_clean'][dog_data_SF_w_tps['topic_11_val_full'] > 0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:03:47.934541Z",
     "start_time": "2019-09-25T03:03:47.919047Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:03:52.372354Z",
     "start_time": "2019-09-25T03:03:49.212499Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps.to_csv(\"Model_Data/dog_data_w_tpcs_3_17_tps.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:03:55.736020Z",
     "start_time": "2019-09-25T03:03:55.731028Z"
    }
   },
   "outputs": [],
   "source": [
    "tags_col = [\n",
    "    \"tags_top_active\",\n",
    "    \"tags_top_loving\",\n",
    "    \"tags_top_playful\",\n",
    "    \"tags_top_sweet\",\n",
    "    \"tags_top_friendly\",\n",
    "]\n",
    "\n",
    "\n",
    "top_row = [\n",
    "    \"topic_1_val\",\n",
    "    \"topic_2_val\",\n",
    "    \"topic_3_val\",\n",
    "    \"topic_4_val\",\n",
    "    \"topic_5_val\",\n",
    "    \"topic_6_val\",\n",
    "    \"topic_7_val\",\n",
    "    \"topic_8_val\",\n",
    "    \"topic_9_val\",\n",
    "    \"topic_10_val\",\n",
    "    \"topic_11_val\",\n",
    "    \"topic_12_val\",\n",
    "    \"topic_13_val\",\n",
    "    \"topic_14_val\",\n",
    "    \"topic_15_val\",\n",
    "    \"topic_16_val\",\n",
    "    \"topic_17_val\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:06:33.731696Z",
     "start_time": "2019-09-25T03:06:33.641391Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps[top_row].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:10:57.381771Z",
     "start_time": "2019-09-25T03:10:57.347054Z"
    }
   },
   "outputs": [],
   "source": [
    "dog_data_SF_w_tps[tags_col].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:13:43.552915Z",
     "start_time": "2019-09-25T03:13:43.521596Z"
    }
   },
   "outputs": [],
   "source": [
    "table = pd.pivot_table(dog_data_SF_w_tps, values= top_row, index= tags_col, aggfunc=np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:13:44.863762Z",
     "start_time": "2019-09-25T03:13:44.830519Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(table,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:29:33.941646Z",
     "start_time": "2019-09-25T03:29:33.933752Z"
    }
   },
   "outputs": [],
   "source": [
    "def topic_tag_test(topic, tag):\n",
    "    contingency = pd.crosstab(dog_data_SF_w_tps[topic], dog_data_SF_w_tps[tag])\n",
    "    c, p, dof, expected = chi2_contingency(contingency)\n",
    "    percent_match = round(\n",
    "        contingency[1][1] / (contingency[1][1] + contingency[1][0]), 3\n",
    "    )\n",
    "    gen_pop_percent = round(\n",
    "        contingency[0][1] / (contingency[0][1] + contingency[0][0]), 3\n",
    "    )\n",
    "    print(contingency)\n",
    "    print(\"P-value \", round(p, 5))\n",
    "    print(\"Percent match: \", percent_match)\n",
    "    print(\"General match: \", gen_pop_percent)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T04:00:30.564959Z",
     "start_time": "2019-09-25T04:00:30.539952Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_17_val', 'tags_top_loving') ### loving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:59:42.740863Z",
     "start_time": "2019-09-25T03:59:42.716185Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_7_val', 'tags_top_friendly')  ### Maybe friendly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:59:42.740863Z",
     "start_time": "2019-09-25T03:59:42.716185Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_10_val', 'tags_top_sweet') ## NOT sweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:59:42.740863Z",
     "start_time": "2019-09-25T03:59:42.716185Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_11_val', 'tags_top_sweet') ## also not sweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:59:42.740863Z",
     "start_time": "2019-09-25T03:59:42.716185Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_9_val', 'tags_top_playful') ## topic 9 and playful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:59:42.740863Z",
     "start_time": "2019-09-25T03:59:42.716185Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_12_val', 'tags_top_playful') ### not playful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T03:59:42.740863Z",
     "start_time": "2019-09-25T03:59:42.716185Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_15_val', 'tags_top_playful') ### similar to playful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-25T04:02:57.247451Z",
     "start_time": "2019-09-25T04:02:57.222904Z"
    }
   },
   "outputs": [],
   "source": [
    "topic_tag_test('topic_10_val', 'tags_top_active') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T03:10:43.676008Z",
     "start_time": "2019-09-28T03:10:43.672210Z"
    }
   },
   "outputs": [],
   "source": [
    "empy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-28T03:11:04.551655Z",
     "start_time": "2019-09-28T03:11:04.543190Z"
    }
   },
   "outputs": [],
   "source": [
    "len(empy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
