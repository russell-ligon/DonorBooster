{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle #for saving output files, pickles\n",
    "\n",
    "def datetime_to_unix_time(d):\n",
    "    return int(time.mktime(datetime.datetime.strptime(d, \"%m/%d/%Y\").timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return \" \".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "True\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n"
     ]
    }
   ],
   "source": [
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy.sql import table, column, select, update, insert\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#In Python: Define your username and password used above. I've defined the database name (we're \n",
    "#using a dataset on births, so I call it birth_db). \n",
    "dbname = 'donors_db'\n",
    "username = 'russell'\n",
    "pswd = 'bradypodion'\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print(engine.url)\n",
    "# Replace localhost with IP address if accessing a remote server\n",
    "\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "print(engine.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process trailers from DonorsChoose (from donors_db in postgreSQL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "\n",
    "### query: from historical data\n",
    "merge_query = \"\"\"\n",
    "SELECT * FROM merge_time;\n",
    "\"\"\"\n",
    "\n",
    "merged_data = pd.read_sql_query(merge_query,con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up project 'hook' text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_hooks(text_str):\n",
    "    if pd.isnull(text_str) != True:\n",
    "        testc1 = strip_tags(text_str)\n",
    "        testc2 = re.sub(\n",
    "            \".?(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?\", \"\", testc1\n",
    "        )\n",
    "        testc3 = testc2.strip(string.punctuation)\n",
    "        textc4 = re.sub(\"\\s+\", \" \", testc3)\n",
    "        textc5 = re.sub(\"[^A-Za-z0-9]+\", \" \", textc4)\n",
    "        textc6 = re.sub(\"\\d+\", \" \", textc5)\n",
    "        return textc6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[\"text\"] = merged_data[\"fulfillmentTrailer\"].apply(clean_hooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My students need an Epson BrightLink  Wi LCD Projector in order to take advantage of learning in a digital world'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "merged_data[\"text\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a length variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_len(text_str):\n",
    "    if pd.isnull(text_str) != True:\n",
    "        return len(text_str)\n",
    "\n",
    "def extract_wordlen(text_str):\n",
    "    if pd.isnull(text_str) != True:\n",
    "        return(len(text_str.split())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[\"len_text\"] = merged_data[\"text\"].apply(extract_len)\n",
    "merged_data[\"num_words\"] = merged_data[\"text\"].apply(extract_wordlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    82090.000000\n",
       "mean        19.193251\n",
       "std          8.062526\n",
       "min          0.000000\n",
       "25%         13.000000\n",
       "50%         18.000000\n",
       "75%         25.000000\n",
       "max        191.000000\n",
       "Name: num_words, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data[\"num_words\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows coming in 82090\n",
      "Row coming out after website/junk filtering 82090\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>index_x</th>\n",
       "      <th>id</th>\n",
       "      <th>proposalURL</th>\n",
       "      <th>fundURL</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>retinaImageURL</th>\n",
       "      <th>thumbImageURL</th>\n",
       "      <th>fulfillmentTrailer</th>\n",
       "      <th>...</th>\n",
       "      <th>date_completed</th>\n",
       "      <th>date_thank_you_packet_mailed</th>\n",
       "      <th>date_expiration</th>\n",
       "      <th>calendar_completed</th>\n",
       "      <th>year_completed</th>\n",
       "      <th>calendar_expired</th>\n",
       "      <th>latency_to_funded</th>\n",
       "      <th>days_to_funding</th>\n",
       "      <th>text</th>\n",
       "      <th>len_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>846450</td>\n",
       "      <td>https://www.donorschoose.org/project/the-gathering-place/846450/</td>\n",
       "      <td>https://secure.donorschoose.org/donors/givingCart.html?proposalid=846450&amp;donationAmount=45</td>\n",
       "      <td>https://www.donorschoose.org/teacher/photo/u481042?size=sm&amp;t=1490717986547</td>\n",
       "      <td>https://www.donorschoose.org/teacher/photo/u481042?size=retina&amp;t=1490717986547</td>\n",
       "      <td>https://www.donorschoose.org/teacher/photo/u481042?size=thmb&amp;t=1490717986547</td>\n",
       "      <td>My students need a rug.</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-09-23 00:00:00</td>\n",
       "      <td>2013-01-29 00:00:00</td>\n",
       "      <td>2013-01-24 00:00:00</td>\n",
       "      <td>2012-09-23</td>\n",
       "      <td>2012</td>\n",
       "      <td>2013-01-24</td>\n",
       "      <td>2246400.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>My students need a rug</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2116678</td>\n",
       "      <td>https://www.donorschoose.org/project/extreme-makeover-needed-for-music-librar/2116678/</td>\n",
       "      <td>https://secure.donorschoose.org/donors/givingCart.html?proposalid=2116678&amp;donationAmount=45</td>\n",
       "      <td>https://www.donorschoose.org/teacher/photo/u1030090?size=sm&amp;t=1510608507481</td>\n",
       "      <td>https://www.donorschoose.org/teacher/photo/u1030090?size=retina&amp;t=1510608507481</td>\n",
       "      <td>https://www.donorschoose.org/teacher/photo/u1030090?size=thmb&amp;t=1510608507481</td>\n",
       "      <td>My students need storage cabinets in our choir room to house our music library.</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-09-29 00:00:00</td>\n",
       "      <td>2016-09-29 00:00:00</td>\n",
       "      <td>2016-12-08 00:00:00</td>\n",
       "      <td>2016-09-29</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-12-08</td>\n",
       "      <td>4406400.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>My students need storage cabinets in our choir room to house our music library</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1892178</td>\n",
       "      <td>https://www.donorschoose.org/project/starting-the-band/1892178/</td>\n",
       "      <td>https://secure.donorschoose.org/donors/givingCart.html?proposalid=1892178&amp;donationAmount=45</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u3033862?size=sm&amp;t=1439495806736</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u3033862?size=retina&amp;t=1439495806736</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u3033862?size=thmb&amp;t=1439495806736</td>\n",
       "      <td>My students need 4 clarinets and 1 trumpet to play in the band.</td>\n",
       "      <td>...</td>\n",
       "      <td>2016-03-09 00:00:00</td>\n",
       "      <td>2016-03-11 00:00:00</td>\n",
       "      <td>2016-06-19 00:00:00</td>\n",
       "      <td>2016-03-09</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>1382400.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>My students need   clarinets and   trumpet to play in the band</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>1143364</td>\n",
       "      <td>https://www.donorschoose.org/project/help-us-buy-music-stands-for-our-rapidly/1143364/</td>\n",
       "      <td>https://secure.donorschoose.org/donors/givingCart.html?proposalid=1143364&amp;donationAmount=45</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u2129201?size=sm&amp;t=1508382071370</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u2129201?size=retina&amp;t=1508382071370</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u2129201?size=thmb&amp;t=1508382071370</td>\n",
       "      <td>My students need 6 new stands at the high school in order to accommodate our rapidly growing program.</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-03 00:00:00</td>\n",
       "      <td>2014-03-26 00:00:00</td>\n",
       "      <td>2014-03-26 00:00:00</td>\n",
       "      <td>2013-12-03</td>\n",
       "      <td>2013</td>\n",
       "      <td>2014-03-26</td>\n",
       "      <td>432000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>My students need   new stands at the high school in order to accommodate our rapidly growing program</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>779936</td>\n",
       "      <td>https://www.donorschoose.org/project/help-students-see-the-world/779936/</td>\n",
       "      <td>https://secure.donorschoose.org/donors/givingCart.html?proposalid=779936&amp;donationAmount=45</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u1397284?size=sm&amp;t=1476116145915</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u1397284?size=retina&amp;t=1476116145915</td>\n",
       "      <td>https://www.donorschoose.org/donors/photo/u1397284?size=thmb&amp;t=1476116145915</td>\n",
       "      <td>My students need a projector to be able to view live performances and other teaching resources from the Internet.</td>\n",
       "      <td>...</td>\n",
       "      <td>2012-07-03 00:00:00</td>\n",
       "      <td>2012-08-22 00:00:00</td>\n",
       "      <td>2012-08-29 00:00:00</td>\n",
       "      <td>2012-07-03</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012-08-29</td>\n",
       "      <td>8035200.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>My students need a projector to be able to view live performances and other teaching resources from the Internet</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  index_x       id  \\\n",
       "0        0      0        0   846450   \n",
       "1        1      7        0  2116678   \n",
       "2        2     19        0  1892178   \n",
       "3        3     25        0  1143364   \n",
       "4        4     36        0   779936   \n",
       "\n",
       "                                                                              proposalURL  \\\n",
       "0                        https://www.donorschoose.org/project/the-gathering-place/846450/   \n",
       "1  https://www.donorschoose.org/project/extreme-makeover-needed-for-music-librar/2116678/   \n",
       "2                         https://www.donorschoose.org/project/starting-the-band/1892178/   \n",
       "3  https://www.donorschoose.org/project/help-us-buy-music-stands-for-our-rapidly/1143364/   \n",
       "4                https://www.donorschoose.org/project/help-students-see-the-world/779936/   \n",
       "\n",
       "                                                                                       fundURL  \\\n",
       "0   https://secure.donorschoose.org/donors/givingCart.html?proposalid=846450&donationAmount=45   \n",
       "1  https://secure.donorschoose.org/donors/givingCart.html?proposalid=2116678&donationAmount=45   \n",
       "2  https://secure.donorschoose.org/donors/givingCart.html?proposalid=1892178&donationAmount=45   \n",
       "3  https://secure.donorschoose.org/donors/givingCart.html?proposalid=1143364&donationAmount=45   \n",
       "4   https://secure.donorschoose.org/donors/givingCart.html?proposalid=779936&donationAmount=45   \n",
       "\n",
       "                                                                      imageURL  \\\n",
       "0   https://www.donorschoose.org/teacher/photo/u481042?size=sm&t=1490717986547   \n",
       "1  https://www.donorschoose.org/teacher/photo/u1030090?size=sm&t=1510608507481   \n",
       "2   https://www.donorschoose.org/donors/photo/u3033862?size=sm&t=1439495806736   \n",
       "3   https://www.donorschoose.org/donors/photo/u2129201?size=sm&t=1508382071370   \n",
       "4   https://www.donorschoose.org/donors/photo/u1397284?size=sm&t=1476116145915   \n",
       "\n",
       "                                                                    retinaImageURL  \\\n",
       "0   https://www.donorschoose.org/teacher/photo/u481042?size=retina&t=1490717986547   \n",
       "1  https://www.donorschoose.org/teacher/photo/u1030090?size=retina&t=1510608507481   \n",
       "2   https://www.donorschoose.org/donors/photo/u3033862?size=retina&t=1439495806736   \n",
       "3   https://www.donorschoose.org/donors/photo/u2129201?size=retina&t=1508382071370   \n",
       "4   https://www.donorschoose.org/donors/photo/u1397284?size=retina&t=1476116145915   \n",
       "\n",
       "                                                                   thumbImageURL  \\\n",
       "0   https://www.donorschoose.org/teacher/photo/u481042?size=thmb&t=1490717986547   \n",
       "1  https://www.donorschoose.org/teacher/photo/u1030090?size=thmb&t=1510608507481   \n",
       "2   https://www.donorschoose.org/donors/photo/u3033862?size=thmb&t=1439495806736   \n",
       "3   https://www.donorschoose.org/donors/photo/u2129201?size=thmb&t=1508382071370   \n",
       "4   https://www.donorschoose.org/donors/photo/u1397284?size=thmb&t=1476116145915   \n",
       "\n",
       "                                                                                                  fulfillmentTrailer  \\\n",
       "0                                                                                            My students need a rug.   \n",
       "1                                    My students need storage cabinets in our choir room to house our music library.   \n",
       "2                                                    My students need 4 clarinets and 1 trumpet to play in the band.   \n",
       "3              My students need 6 new stands at the high school in order to accommodate our rapidly growing program.   \n",
       "4  My students need a projector to be able to view live performances and other teaching resources from the Internet.   \n",
       "\n",
       "   ...       date_completed  date_thank_you_packet_mailed  \\\n",
       "0  ...  2012-09-23 00:00:00           2013-01-29 00:00:00   \n",
       "1  ...  2016-09-29 00:00:00           2016-09-29 00:00:00   \n",
       "2  ...  2016-03-09 00:00:00           2016-03-11 00:00:00   \n",
       "3  ...  2013-12-03 00:00:00           2014-03-26 00:00:00   \n",
       "4  ...  2012-07-03 00:00:00           2012-08-22 00:00:00   \n",
       "\n",
       "       date_expiration  calendar_completed year_completed  calendar_expired  \\\n",
       "0  2013-01-24 00:00:00          2012-09-23           2012        2013-01-24   \n",
       "1  2016-12-08 00:00:00          2016-09-29           2016        2016-12-08   \n",
       "2  2016-06-19 00:00:00          2016-03-09           2016        2016-06-19   \n",
       "3  2014-03-26 00:00:00          2013-12-03           2013        2014-03-26   \n",
       "4  2012-08-29 00:00:00          2012-07-03           2012        2012-08-29   \n",
       "\n",
       "   latency_to_funded days_to_funding  \\\n",
       "0          2246400.0            26.0   \n",
       "1          4406400.0            51.0   \n",
       "2          1382400.0            16.0   \n",
       "3           432000.0             5.0   \n",
       "4          8035200.0            93.0   \n",
       "\n",
       "                                                                                                               text  \\\n",
       "0                                                                                            My students need a rug   \n",
       "1                                    My students need storage cabinets in our choir room to house our music library   \n",
       "2                                                    My students need   clarinets and   trumpet to play in the band   \n",
       "3              My students need   new stands at the high school in order to accommodate our rapidly growing program   \n",
       "4  My students need a projector to be able to view live performances and other teaching resources from the Internet   \n",
       "\n",
       "  len_text  \n",
       "0       22  \n",
       "1       78  \n",
       "2       62  \n",
       "3      100  \n",
       "4      112  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########drop rows that have www or donorchoose in their fulfillmentTrailers---the wrong info was grabbed\n",
    "print('Rows coming in '+str(len(merged_data.index)))\n",
    "merged_data= merged_data[~merged_data.text.str.contains(\"www\")]\n",
    "merged_data= merged_data[~merged_data.text.str.contains(\"donorschoose\")]\n",
    "print('Row coming out after website/junk filtering '+str(len(merged_data.index)))\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not using re.sub(r'[^a-zA-Z\\s]', '', t) to avoid losing emojis\n",
    "text = [re.sub(r'([0-9]+?)', ' ', t).lower() for t in data['text']] # remove all numbers and symbols\n",
    "text = [re.sub(r'(!|\"|#|\\$|%|&|\\'|\\(|\\)|\\*|\\+|,|-|\\.|/|:|;|<|=|>|\\?|@|\\[|\\\\|\\]|\\^|_|`|{|\\||}|~)+', ' ', t) for t in text]\n",
    "data['text'] = [re.sub(r'\\s+\\s', ' ', t).strip() for t in text] # repace double spaces with single spaces\n",
    "\n",
    "data = data.loc[data['text'].map(len) > 3].reset_index(drop = True) # keep only strings longer than 5 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovering and Visualizing Topics in Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most typical cases of text classification in NLP (named entity recognition, question answering, etc) require training datasets where each piece of text is associted with a label. However, in real-life scenarios, text collections rarely come with metadata labels that tell you what the texts are about. When people answer open-ended survey questions, for example, they don't repeat detectable keywords in their answer with the topics they discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic modeling** is an unsupervised classification technique that is able to discover the topics in a collection of texts by looking at their commonalities. In this context, \"topics\" refers to groups of related words that often occur together in the same text. For example, in a collection of newspaper articles a topic model may identify one topic that is made up of words such as \"politician\", \"law\", and \"parliament\", and another characterized by words such as \"player\", \"match\" and \"penalty\". Topic models only go as far as identifying clusters of related words; a human is still needed to interpret these clusters and give them labels such as \"politics\" and \"football\". \n",
    "\n",
    "One of the most popular topic models is Latent Dirichlet Allocation (LDA). LDA is a generative model that sees every text as a mixture of topics and each sentence as a mixture of words. For example, the \"football\" topic will generate the word \"penalty\" with a high probability, while the \"politics\" topic will have a much higher probability for \"politician\" than for \"penalty\". Other words, such as \"the\" and \"an\", will have similar probabilities in all topics. LDA takes its name from the Dirichlet probability distribution. This is the prior distribution it assumes the topics in a text will have.\n",
    "\n",
    "Modified from https://github.com/nlptown/nlp-notebooks/blob/master/Discovering%20and%20Visualizing%20Topics%20in%20Texts%20with%20LDA.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight fellows frequently come up with project ideas that revolve around topic modeling of online reviews. Here, we'll use a dataset of project 'trailers' from the website DonorsChoose providing a brief description of the reason/project for which a teacher is requesting funding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before we train a topic model, we need to tokenize our texts. Let's do this with the [spaCy](https://spacy.io/) NLP library. We need to load a statisti English and use spaCy to perform our first preprocessing pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# If you haven't installed the spaCy language model, uncomment the following line and run this cell\n",
    "#! python -m spacy download en_core_web_sm\n",
    "\n",
    "# You will need to restart the notebook (go to the menu Kernel -> Restart) and re-run cells up to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "texts = data['text'].tolist()\n",
    "%time spacy_docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text of each review is now a spaCy Doc that we can transform into a list of tokens. Instead of the original tokens, we're going to work with the **lemmas** instead. This will allow our model to generalize and understand that different forms of a word should be treated as one.\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the words. Lemmatization uses the rules about a language and the resulting tokens are all actual words. For example, the word \"thought\" becomes the lemma \"think\". Stemming is a crude heuristic that chops off the ends of words such that the resulting tokens may not be actual words. Stemming is faster but only works well for simple words like \"toys\" and \"toy\".\n",
    "\n",
    "This is the full list of our initial preprocessing steps: \n",
    " \n",
    "- remove all words shorter than 2 characters (these are often fairly uninteresting from a topical point of view)\n",
    "- drop all stopwords\n",
    "- lowercase remaining lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [[t.lemma_.lower() for t in doc if len(t.orth_) > 2 and not t.is_stop] for doc in spacy_docs]\n",
    "for i in range(5):\n",
    "    print(docs[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we also want to take frequent bigrams into account. **Bigrams are multiword units**, such as \"colored pencil\" that actually form one word rather than two. We'll use Gensim to first identify the frequent bigrams in the corpus, then append them to the list of tokens for the documents in which they appear. This means the bigrams will not be in their correct position in the text, but that's fine: topic models are bag-of-word models that ignore word position anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "tokens = []\n",
    "\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:  # bigrams can be recognized by the \"_\" that joins the invidual words\n",
    "            docs[idx].append(token)\n",
    "            tokens.append(token)\n",
    "            \n",
    "print(list(set(tokens))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_out = open('/home/russell/Documents/GitHub/DonorBooster/cleantrailersbig.pickle',\"wb\")\n",
    "pickle.dump(docs, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open('/home/russell/Documents/GitHub/DonorBooster/cleantokensbig.pickle',\"wb\")\n",
    "pickle.dump(tokens, pickle_out)\n",
    "pickle_out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we move on to the final Gensim-specific preprocessing steps. First, we create a dictionary representation of the documents. This dictionary will map each word to a unique ID and help us create bag-of-word representations of each document. These bag-of-word representations contain the ids of the words in the document, together with their frequency. Additionally, we can remove the least and most frequent words from the vocabulary. This improves the quality of our topic model and speeds up its training. The minimum frequency of a word is expressed as an absolute number, the maximum frequency is the proportion of documents a word is allowed to occur in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "print('Number of unique words in original documents:', len(dictionary))\n",
    "\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.25)\n",
    "print('Number of unique words after removing rare and common words:', len(dictionary))\n",
    "\n",
    "print(\"Example representation of document 3:\", dictionary.doc2bow(docs[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create bag-of-word representations for each document in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Now it's time to train our topic model. We do this with the following parameters: \n",
    "\n",
    "- `corpus`: the bag-of-word representations of our documents\n",
    "- `id2token`: the mapping from indices to words\n",
    "- `num_topics`: the number of topics we want the model to identify\n",
    "- `chunksize`: the number of documents the model sees for every update\n",
    "- `passes`: the number of times we show the total corpus to the model during training\n",
    "- `random_state`: we use a seed to ensure reproducibility.\n",
    "\n",
    "On a corpus of this size, the training will typically about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "%time model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=20, chunksize=500, passes=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the model has learnt. We do this by printing out the ten words that are most characteristic for each of the topics. Most topics show common words like \"experience\", \"item\" and \"school\" but it's hard to identify any other patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (topic, words) in model.print_topics():\n",
    "    print(topic+1, \":\", words, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of inspecting the topics is by visualizing them. This can be done with the [pyLDAvis](https://github.com/bmabey/pyLDAvis) library. PyLDAvis will show us how popular the topics are in our corpus, how similar the topics are, and which are the most salient words for this topic. Note it's important to set `sort_topics=False` on the call to pyLDAvis. If you don't, it will order the topics differently than Gensim. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "pyLDAvis.gensim.prepare(model, corpus, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's inspect the topics the model recognizes in some of the individual documents. Here we see how LDA tends to assign a high probability to a low number of topics for each documents, which makes its results easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (text, doc) in zip(texts, docs):\n",
    "    val=([(topic+1, prob) for (topic, prob) in model[dictionary.doc2bow(doc)] if ((prob > 0.5) and topic==8)])\n",
    "       \n",
    "    if len(val)!=0:\n",
    "        print(text)\n",
    "        print('-'*10)\n",
    "        print(val)\n",
    "        \n",
    "\n",
    "    del val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for (text, doc) in zip(texts[:20], docs[:20]):\n",
    "    print(text)\n",
    "    print('-'*10)\n",
    "    print([(topic+1, prob) for (topic, prob) in model[dictionary.doc2bow(doc)] if prob > 0.7])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nums = []\n",
    "for (text, doc) in zip(texts, docs):\n",
    "    probs = np.array(model[dictionary.doc2bow(doc)])\n",
    "    topic_nums.append(probs[np.argsort(probs[:,-1])][-1,0])\n",
    "    \n",
    "data['topic'] = topic_nums\n",
    "\n",
    "product_vs_topic = pd.crosstab(data['id'], data['topic'])\n",
    "product_vs_topic = product_vs_topic.T / product_vs_topic.sum(axis = 1) * 100\n",
    "product_vs_topic = product_vs_topic.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option(\"max_rows\", None) #undo by resetting --- \n",
    "#pd.reset_option(\"display.max_rows\")\n",
    "#pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "def background_gradient(s, m, M, cmap='PuBu', low=0, high=0):\n",
    "    rng = M - m\n",
    "    norm = colors.Normalize(m - (rng * low),\n",
    "                            M + (rng * high))\n",
    "    normed = norm(s.values)\n",
    "    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n",
    "    return ['background-color: %s' % color for color in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "product_vs_topic.round(2).style.apply(background_gradient,\n",
    "               cmap='YlGnBu',\n",
    "               m=product_vs_topic.min().min(),\n",
    "               M=product_vs_topic.max().max(),\n",
    "               low=0.5,\n",
    "               high=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['id'].isin(['4957430','4957502','4957562'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many collections of unstructured texts don't come with any labels. Topic models such as Latent Dirichlet Allocation are a useful technique to discover the most prominent topics in such documents. Gensim makes training these topics model easy, and pyLDAvis presents the results in a visually attractive way. Together they form a powerful toolkit to better understand what's inside large sets of documents and to explore subsets of related texts. However, these methods can perform poorly in short texts with vague or unspecified subjects. Although traditional topic models are lacking in more semantic information (they don't use word embeddings, for instance), they can be really quick way of getting insights into large collections of documents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
