{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:03:55.740214Z",
     "start_time": "2019-10-01T17:03:49.337675Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Call my other functions\n",
    "from DonorsChooseFunx import *\n",
    "\n",
    "# Core\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import datetime as dt\n",
    "import sys\n",
    "import imgkit\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics \n",
    "import string\n",
    "\n",
    "import lifelines\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.datasets import load_rossi\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta, date #for time duration calculations\n",
    "from dateutil.parser import parse #for fuzzy finding year\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "True\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n"
     ]
    }
   ],
   "source": [
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy.sql import table, column, select, update, insert\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#In Python: Define your username and password used above. I've defined the database name (we're \n",
    "#using a dataset on births, so I call it birth_db). \n",
    "dbname = 'donors_db'\n",
    "username = 'russell'\n",
    "pswd = 'bradypodion'\n",
    "\n",
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print(engine.url)\n",
    "# Replace localhost with IP address if accessing a remote server\n",
    "\n",
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "print(engine.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DC\", \"DE\", \"FL\", \"GA\", \n",
    "          \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "          \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "          \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \n",
    "          \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [05:44<00:00,  6.75s/it]\n"
     ]
    }
   ],
   "source": [
    "#loop through states, merge from historical and scraped data\n",
    "#\n",
    "\n",
    "stateset = []\n",
    "\n",
    "for stateval in tqdm(states):\n",
    "    ###########################################################\n",
    "    ### query: from historical data\n",
    "    hist_query = \"\"\"\n",
    "    SELECT * FROM hist_projects WHERE school_state='\"\"\"+stateval+\"';\\n\"\n",
    "\n",
    "\n",
    "    hist_state = pd.read_sql_query(hist_query,con)\n",
    "    orig_hist_rows = len(hist_state.index)\n",
    "    hist_state = hist_state.drop_duplicates(keep='first')\n",
    "    dedup_hist_rows = len(hist_state.index)\n",
    "    \n",
    "    hist_state['projectover'] = hist_state.apply(lambda row: projectover(row['date_posted'],row['date_completed'],row['calendar_expired']),axis=1)\n",
    "    hist_state['starting'] = hist_state.apply(lambda row: makedate(row['date_posted']),axis=1)\n",
    "    hist_state['days_to_funding'] = hist_state.apply(lambda row: elapseddays(row['starting'],row['projectover']).days,axis=1)\n",
    "    #####\n",
    "    j5=[];k5=[];r5=[]\n",
    "    for x in hist_state['school_metro']:\n",
    "        if x is None:\n",
    "            y = 'none'\n",
    "        else:\n",
    "            y = x       \n",
    "        j5.append(y)\n",
    "\n",
    "    for x in hist_state['resource_type']:\n",
    "        if x is None:\n",
    "            y = 'none'\n",
    "        else:\n",
    "            y = x   \n",
    "        k5.append(y)\n",
    "\n",
    "    for x in hist_state['students_reached']:\n",
    "        if x is None:\n",
    "            y = 0\n",
    "        elif x=='null':\n",
    "            y = 0\n",
    "        else:\n",
    "            y = float(x)    \n",
    "        r5.append(y)\n",
    "\n",
    "    hist_state['school_metro']=j5\n",
    "    hist_state['resource_type']=k5\n",
    "    hist_state['students_reached']=r5\n",
    "    \n",
    "    \n",
    "\n",
    "    unique_posting_dts = hist_state.starting.unique()#for each unique date\n",
    "    dc = hist_state.projectover\n",
    "    dp = hist_state.starting\n",
    "    active_projects=[]\n",
    "    for upd in unique_posting_dts:\n",
    "        active_projects.append(((upd>dp) & (upd<=dc)).sum())\n",
    "        \n",
    "    matched_dict = dict(zip(unique_posting_dts, active_projects))\n",
    "    hist_state['n_active_at_posting']=hist_state['starting'].map(matched_dict)\n",
    "        \n",
    "    #print(stateval+': ' +str(len(hist_state.index))++,)\n",
    "    stateset.append(hist_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = pd.concat(stateset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1425163, 51)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', '_projectid', '_teacher_acctid', '_schoolid', 'school_ncesid',\n",
       "       'school_latitude', 'school_longitude', 'school_city', 'school_state',\n",
       "       'school_zip', 'school_metro', 'school_district', 'school_county',\n",
       "       'school_charter', 'school_magnet', 'school_year_round', 'school_nlns',\n",
       "       'school_kipp', 'school_charter_ready_promise', 'teacher_prefix',\n",
       "       'teacher_teach_for_america', 'primary_focus_subject',\n",
       "       'primary_focus_area', 'secondary_focus_subject', 'secondary_focus_area',\n",
       "       'resource_type', 'poverty_level', 'grade_level',\n",
       "       'vendor_shipping_charges', 'sales_tax', 'payment_processing_charges',\n",
       "       'fulfillment_labor_materials', 'total_price_excluding_optional_support',\n",
       "       'total_price_including_optional_support', 'students_reached',\n",
       "       'total_donations', 'num_donors', 'eligible_double_your_impact_match',\n",
       "       'eligible_almost_home_match', 'funding_status', 'date_posted',\n",
       "       'date_completed', 'date_thank_you_packet_mailed', 'date_expiration',\n",
       "       'calendar_completed', 'year_completed', 'calendar_expired',\n",
       "       'projectover', 'starting', 'days_to_funding', 'n_active_at_posting'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data.to_sql('state_data_active', engine, if_exists='append',chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:04:02.309147Z",
     "start_time": "2019-10-01T17:04:02.252247Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_data['cal_month'].value_counts()\n",
    "\n",
    "good_data['pMonth_x'] = good_data.apply(lambda row: Convert_to_clock_x(row.posting_month),axis=1)\n",
    "good_data['pMonth_y'] = good_data.apply(lambda row: Convert_to_clock_y(row.posting_month),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOOL to determine active projects at the time a given project was posted.\n",
    "#if all already in df.Timestamp format ELSE pd.Timestamp(x)\n",
    "\n",
    "#dp = hist_state.date_posted\n",
    "#dc = hist_state.projectover\n",
    "#hist_state['nactive'] = dp.apply(lambda x: ((x>dp) & (x<=dc)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['posting_day']=good_data['posting_day'].str.split(\" \").str[0]\n",
    "good_data['posting_day']=pd.to_numeric(good_data['posting_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['posting_day'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['postingstring']=good_data['posting_year'].astype(str)+'_'+good_data['posting_month'].astype(str)+'_'+good_data['posting_day'].astype(str)\n",
    "good_data['projectover'] = good_data.apply(lambda row: projectover(row['date_posted'],row['date_completed'],row['date_expiration']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace date time with date object\n",
    "#good_data['projectover'] = good_data.apply(lambda row:row['date_posted'].date(),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = good_data.sort_values(by=['posting_year','posting_month','posting_day'], ascending=True)\n",
    "sorted_df['date_posted']=pd.to_datetime(sorted_df['date_posted'])\n",
    "\n",
    "#replace date time with date object\n",
    "sorted_df['date_posted'] = sorted_df.apply(lambda row: datetime.date(row['date_posted'].year, row['date_posted'].month, row['date_posted'].day),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_posting_days = sorted_df.postingstring.unique()\n",
    "unique_posting_dts = sorted_df.date_posted.unique()\n",
    "print(len(unique_posting_dts))\n",
    "print(unique_posting_dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = sorted_df.projectover\n",
    "dp = sorted_df.date_posted\n",
    "active_projects=[]\n",
    "\n",
    "for upd in tqdm(unique_posting_dts):\n",
    "    active_projects.append(((upd>dp) & (upd<=dc)).sum())\n",
    "    #dp.apply(lambda x: ((x>dp) & (x<=dc)).sum())\n",
    "\n",
    "#dp = hist_state.date_posted\n",
    "#dc = hist_state.projectover\n",
    "#hist_state['nactive'] = dp.apply(lambda x: ((x>dp) & (x<=dc)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_dict = dict(zip(unique_posting_dts, active_projects))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['n_active_at_posting']=sorted_df['date_posted'].map(matched_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['totalPrice'] = sorted_df['totalPrice'].astype(float)\n",
    "sorted_df['totalPrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postedDATE = sorted_df.date_posted\n",
    "hold = sorted_df.drop(['date_posted'], axis=1)\n",
    "#recombine with date_posted as last column\n",
    "sorted_df = pd.concat([hold, postedDATE], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdon=sorted_df #temp holding frame\n",
    "holdon.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted_df = holdon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add rows for missing dates\n",
    "r = pd.date_range(start=sorted_df.date_posted.min(), end=sorted_df.date_posted.max())\n",
    "r2=pd.DataFrame(r)\n",
    "r2=r2.rename(columns={0:'fulltime'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filler = pd.Series(['NaN']*101,index=sorted_df.columns)\n",
    "\n",
    "#\n",
    "listforadding=[]\n",
    "for checkdate in tqdm(r2['fulltime']):\n",
    "    if (checkdate not in sorted_df.date_posted.values):\n",
    "        filler['date_posted']=checkdate\n",
    "        rowtoadd = pd.DataFrame(filler).transpose()\n",
    "        listforadding.append(rowtoadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsforadding = pd.concat(listforadding)\n",
    "rowsforadding.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    sorted_df[[\"primary_focus_area\", \"n_active_at_posting\",\"cal_month\"]],\n",
    "    columns=[\"cal_month\"],\n",
    "    aggfunc=np.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df = sorted_df.append(pd.DataFrame(data = rowsforadding), ignore_index=True)\n",
    "# sorted_df = sorted_df.sort_values(by=['date_posted'], ascending=True)\n",
    "# print(sorted_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "plt.scatter(sorted_df.n_active_at_posting,sorted_df.days_to_funding, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "sns.stripplot(sorted_df.pMonth_x,sorted_df.pMonth_y, s=30,alpha=0.0021,jitter=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augustfirst='-08-01'\n",
    "augs=[]\n",
    "for y in range(2003,2017,1):\n",
    "    scstrt = datetime.datetime.strptime(str(y)+augustfirst,'%Y-%m-%d')\n",
    "    augs.append(scstrt)\n",
    "augs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['posting_year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "# create figure and axis objects with subplots()\n",
    "fig,ax = plt.subplots()\n",
    "# make a plot\n",
    "ax.plot(sorted_df.date_posted, sorted_df[\"days_to_funding\"],alpha=0.021,color=\"blue\",marker=\"o\")\n",
    "#set x-axis limits\n",
    "ax.set_xlim(r[0],r[-1])\n",
    "\n",
    "# set x-axis label\n",
    "ax.set_xlabel(\"year\",fontsize=14)\n",
    "# set y-axis label\n",
    "ax.set_ylabel(\"Days to funding\",color=\"blue\",fontsize=14)\n",
    "\n",
    "\n",
    "# twin object for two different y-axis on the sample plot\n",
    "ax2=ax.twinx()\n",
    "# make a plot with different y-axis using second axis object\n",
    "ax2.plot(sorted_df.date_posted, sorted_df.n_active_at_posting,alpha=0.81, color=\"red\")\n",
    "ax2.set_ylabel(\"Number active projects\",color=\"red\",fontsize=14)\n",
    "\n",
    "for p in augs:\n",
    "    plt.axvline(p,  label='pyplot vertical line',color='black')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "# save the plot as a file\n",
    "fig.savefig('ActiveProjects & Time-to-fund over time.png',\n",
    "            format='png',\n",
    "            dpi=300,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "#plt.scatter(sorted_df.date_posted, sorted_df.n_active_at_posting,alpha=0.021);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# clean up poverty level\n",
    "sorted_df[\"poverty_clean\"] = sorted_df[\"poverty_level_y\"].str.replace(\" poverty\", \"\")\n",
    "sorted_df['poverty_clean'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up focus level\n",
    "sorted_df[\"primary_focus_subject\"] = sorted_df[\"primary_focus_subject\"].str.replace(\" \", \"_\")\n",
    "sorted_df['primary_focus_subject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't start over\n",
    "# pickle_out = open('/home/russell/Documents/DataScience/DonorsChoose/Data/sorted_df.pickle',\"wb\")\n",
    "# pickle.dump(sorted_df, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1HOT encode relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "print(features.shape)\n",
    "features = features.dropna()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resp = composite['days_to_funding']\n",
    "\n",
    "sns.set_context(\"poster\", font_scale=1.3)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.distplot(resp.dropna())\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sqrt_resp = resp**(.5)\n",
    "sns.distplot(sqrt_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recip = 1/resp\n",
    "sns.distplot(recip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_resp = np.log(resp)\n",
    "sns.distplot(log_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "bxp_resp, fitted_lambda = stats.boxcox(resp)\n",
    "print(fitted_lambda)\n",
    "sns.distplot(bxp_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The baseline predictions are the historical averages\n",
    "baseline_preds = x_test[:, feature_list.index('average')]\n",
    "\n",
    "#Baseline errors, and display average baseline error\n",
    "baseline_errors = abs(baseline_preds - y_test)\n",
    "print('Average baseline error: ', round(np.mean(baseline_errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (with states) BOXCOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, fitted_lambda = stats.boxcox(composite.days_to_funding)\n",
    "\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "invbx= inv_boxcox(np.mean(errors),0.1795925846427111)\n",
    "print('Mean Absolute Error:', round(invbx, 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "inv_boxcox(rmse,0.1795925846427111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "i_y_pred=inv_boxcox(y_pred,0.1795925846427111)\n",
    "i_y_test=inv_boxcox(y_test,0.1795925846427111)\n",
    "\n",
    "plt.scatter(i_y_pred,i_y_test, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (withOUT states) BOXCOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting',\n",
    "                  'num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "print(features.shape)\n",
    "features = features.dropna()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "composite.columns\n",
    "#composite=composite[composite.days_to_funding > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = composite.days_to_funding\n",
    "y, fitted_lambda = stats.boxcox(composite.days_to_funding)\n",
    "\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf2 = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf2.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf2.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "invbx= inv_boxcox(np.mean(errors),0.1795925846427111)\n",
    "print('Mean Absolute Error:', round(invbx, 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "inv_boxcox(rmse,0.1795925846427111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf2.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf2\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "i_y_pred=inv_boxcox(y_pred,0.1795925846427111)\n",
    "i_y_test=inv_boxcox(y_test,0.1795925846427111)\n",
    "plt.xlim(0, 140)\n",
    "\n",
    "plt.scatter(i_y_pred,i_y_test, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model BOXCOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "\n",
    "\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n",
    "composite = composite.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = composite.days_to_funding\n",
    "y, fitted_lambda = stats.boxcox(composite.days_to_funding)\n",
    "\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    reg_lambda=1,\n",
    "    gamma=0,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(regressor.feature_importances_.reshape(1, -1), columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "inv_boxcox(rmse,0.1795925846427111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ************* *********** ********  ***************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (with states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "\n",
    "plt.scatter(y_pred,y_test, alpha=0.081);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regression (withOUT states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y',\n",
    "                  'num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "print(features.shape)\n",
    "features = features.dropna()\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model \n",
    "rf2 = RandomForestRegressor(n_estimators= 100, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf2.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "y_pred = rf2.predict(x_test)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(y_pred - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'days')\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf2.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[0:10]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=rf2\n",
    "\n",
    "feature_importance = importances\n",
    "feature_importance = 100.0 * (feature_importance / max(feature_importance))\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(20, 15))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "featax.set_yticks(pos)\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib for plotting and use magic command for Jupyter Notebooks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20,10)\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_list, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "\n",
    "\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n",
    "composite = composite.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = xgb.XGBRegressor(\n",
    "    n_estimators=100,\n",
    "    reg_lambda=1,\n",
    "    gamma=0,\n",
    "    max_depth=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(regressor.feature_importances_.reshape(1, -1), columns=feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Search CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=sorted_df[['days_to_funding','numStudents','totalPrice',\n",
    "                  'n_active_at_posting','pMonth_x','pMonth_y','posting_school_month',\n",
    "                  'len_text','num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(sorted_df[['state','grade_level_x','school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean','cal_month']])\n",
    "\n",
    "\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "\n",
    "#drop projects that are funded in less than 1 day\n",
    "composite=composite[composite.days_to_funding > 0]\n",
    "\n",
    "composite = composite.dropna()\n",
    "\n",
    "y = composite.days_to_funding\n",
    "\n",
    "#\n",
    "x = composite.drop(['days_to_funding'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)\n",
    "\n",
    "# Split features and target into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_scaled, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf3a = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf3a_random = RandomizedSearchCV(estimator = rf3a, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf3a_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Create a based model\n",
    "rf3b = RandomForestRegressor()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = sorted_df[sorted_df.days_to_funding > 0]\n",
    "\n",
    "\n",
    "trimmed=holder[['numStudents','totalPrice',\n",
    "                  'n_active_at_posting',\n",
    "                  'num_words']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(holder[['school_metro',\n",
    "                  'primary_focus_subject','resource_type','poverty_clean']])\n",
    "\n",
    "threeweeks=1*(holder['days_to_funding']<22)\n",
    "\n",
    "good_month=np.where((holder['cal_month']=='b_feb')|(holder['cal_month']=='c_mar')|(holder['cal_month']=='h_aug')|(holder['cal_month']=='j_oct'),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['good_month']=good_month\n",
    "trimmed['threeweeks']=threeweeks\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "composite = composite.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite['threeweeks'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.threeweeks\n",
    "#\n",
    "x = composite.drop(['threeweeks'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "\n",
    "x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "logistic_regression = LogisticRegression()\n",
    "model = logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_regression.predict(x_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "accuracy_percentage = 100 * accuracy\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = model.score(x_test, y_test)\n",
    "print(score)\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'magma');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib\n",
    "\n",
    "# font = {'family' : 'normal',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 30}\n",
    "\n",
    "# matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams.update({'font.size': 42})\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, logistic_regression.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic_regression.predict_proba(x_test)[:,1])\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc,linewidth=4)\n",
    "plt.plot([0, 1], [0, 1],'r--',linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.columns)\n",
    "print(logistic_regression.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=logistic_regression\n",
    "\n",
    "feature_importance = abs(clf.coef_[0])\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "sorted_idx\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos[-10:], feature_importance[sorted_idx][-10:], align='center')\n",
    "featax.set_yticks(pos[-10:])\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx][-10:], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get importance\n",
    "importance = logistic_regression.coef_[0]\n",
    "nombres=x.columns\n",
    "nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplified Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holder = sorted_df[sorted_df.days_to_funding > 0]\n",
    "\n",
    "\n",
    "trimmed=holder[['n_active_at_posting','totalPrice']]\n",
    "\n",
    "# One-hot encode categorical features\n",
    "features = pd.get_dummies(holder[['school_metro',\n",
    "                  'primary_focus_subject','resource_type']])\n",
    "\n",
    "threeweeks=1*(holder['days_to_funding']<22)\n",
    "\n",
    "good_month=np.where((holder['cal_month']=='b_feb')|(holder['cal_month']=='c_mar')|(holder['cal_month']=='h_aug')|(holder['cal_month']=='j_oct'),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['good_month']=good_month\n",
    "trimmed['threeweeks']=threeweeks\n",
    "\n",
    "composite = pd.concat([trimmed,features],axis=1)\n",
    "composite = composite.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite=composite[['threeweeks',\n",
    "                     'n_active_at_posting', \n",
    "                     'totalPrice', \n",
    "                     'good_month',\n",
    "                     'school_metro_urban',\n",
    "                     'primary_focus_subject_Health_&_Wellness',\n",
    "                     'primary_focus_subject_Nutrition',\n",
    "                     'resource_type_Trips',\n",
    "                     'resource_type_Technology',\n",
    "                     'resource_type_Books']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "composite['threeweeks'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = composite.threeweeks\n",
    "#\n",
    "x = composite.drop(['threeweeks'], axis=1)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(x.columns)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "\n",
    "x_scaled = scaler.transform(x)\n",
    "#x_scaled = preprocessing.scale(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.20)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "logistic_regression = LogisticRegression()\n",
    "model = logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_regression.predict(x_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "accuracy_percentage = 100 * accuracy\n",
    "accuracy_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use score method to get accuracy of model\n",
    "score = model.score(x_test, y_test)\n",
    "print(score)\n",
    "\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'magma');\n",
    "plt.ylabel('Actual label');\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0}'.format(score)\n",
    "plt.title(all_sample_title, size = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib\n",
    "\n",
    "# font = {'family' : 'normal',\n",
    "#         'weight' : 'bold',\n",
    "#         'size'   : 30}\n",
    "\n",
    "# matplotlib.rc('font', **font)\n",
    "matplotlib.rcParams.update({'font.size': 42})\n",
    "\n",
    "logit_roc_auc = roc_auc_score(y_test, logistic_regression.predict(x_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic_regression.predict_proba(x_test)[:,1])\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc,linewidth=4)\n",
    "plt.plot([0, 1], [0, 1],'r--',linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=logistic_regression\n",
    "\n",
    "feature_importance = abs(clf.coef_[0])\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "sorted_idx\n",
    "\n",
    "plt.rcParams.update({'font.size': 42})\n",
    "plt.figure(figsize=(24, 16))\n",
    "featfig = plt.figure()\n",
    "featax = featfig.add_subplot(1, 1, 1)\n",
    "featax.barh(pos[-10:], feature_importance[sorted_idx][-10:], align='center')\n",
    "featax.set_yticks(pos[-10:])\n",
    "featax.set_yticklabels(np.array(x.columns)[sorted_idx][-10:], fontsize=18)\n",
    "featax.set_xlabel('Relative Feature Importance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting whether a project will be funded within 3 weeks\n",
    "\n",
    "\n",
    "\n",
    "now_active = 5000 # how many projects are active *RIGHT* now\n",
    "totalPrice = 50\n",
    "goodmonth = 1\n",
    "school_metro_urban = 1\n",
    "primary_focus_HW = 0\n",
    "primary_focus_Nut = 0\n",
    "resource_type_Trip = 0\n",
    "resource_type_Tech = 0\n",
    "resource_type_Book =1\n",
    "\n",
    "valuearray=np.array([[now_active,totalPrice,goodmonth,school_metro_urban,primary_focus_HW,\n",
    "            primary_focus_Nut,resource_type_Trip,resource_type_Tech,resource_type_Book]])\n",
    " \n",
    "valuearray=scaler.transform(valuearray)\n",
    "print(valuearray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FundedFast = logistic_regression.predict_proba((valuearray.reshape(1, -1)))\n",
    "print(\"Likelihood of getting funded with 3 weeks of posting = \"+str(FundedFast[0][1])) # Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# SAVE MODEL TO DISK\n",
    "# Save to file in the current working directory\n",
    "pkl_filename = \"simplified_logistic_regression_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(logistic_regression, file)\n",
    "    \n",
    "#SAVE scaler too\n",
    "pkl2_filename = \"simplified_logistic_regression_scaler.pkl\"\n",
    "with open(pkl2_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.fit(iris.data, iris.target)\n",
    "# Extract single tree\n",
    "estimator = model.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = iris.feature_names,\n",
    "                class_names = iris.target_names,\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['formattedstart']=pd.to_datetime(sorted_df['date_posted'])\n",
    "\n",
    "GB=sorted_df.groupby([(sorted_df['formattedstart'].dt.year.values),(sorted_df['formattedstart'].dt.month.values)]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB.plot('n_active_at_posting','days_to_funding',kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-01T17:04:58.315745Z",
     "start_time": "2019-10-01T17:04:58.288709Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(\n",
    "    good_data[[\"primary_focus_area\", \"days_to_funding\",\"cal_month\"]],\n",
    "    columns=[\"primary_focus_area\"],\n",
    "    aggfunc=np.mean\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_dist(df_to_use, cat_to_subset, var_dist, figw,figh,linew):\n",
    "    plt.figure(figsize=(figw,figh))\n",
    "    sns.set_context( rc={\"lines.linewidth\": linew})\n",
    "    \n",
    "    for grp in sorted(df_to_use[cat_to_subset].unique()):\n",
    "        grp_df = df_to_use.loc[df_to_use[cat_to_subset] == grp]\n",
    "        \n",
    "        sns.distplot(grp_df[var_dist], hist=False, label=grp)\n",
    "        plt.xlim(0, 90)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['school_metro'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"primary_focus_area\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"cal_month\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"school_metro\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"resource_type\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"poverty_clean\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"grade_level_y\", \"days_to_funding\",12,10,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_dist(sorted_df, \"posting_year\", \"days_to_funding\",12,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data =good_data.set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['days_to_funding'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data['time_fund_med'] =1*(good_data['days_to_funding'] < 7)\n",
    "good_data['time_fund_med'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data cleaning / feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:05.547616Z",
     "start_time": "2019-10-15T03:04:05.472392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "good_data['len_text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:06.819576Z",
     "start_time": "2019-10-15T03:04:06.715715Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_data['min_desc'] = 0\n",
    "good_data['min_desc'][good_data['len_text'] <126] =1\n",
    "\n",
    "good_data['min_desc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:07.698796Z",
     "start_time": "2019-10-15T03:04:07.665704Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = good_data.dropna(subset=['len_text'])\n",
    "good_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:09.369392Z",
     "start_time": "2019-10-15T03:04:09.349782Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Categorical.describe(good_data['primary_focus_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:10.964962Z",
     "start_time": "2019-10-15T03:04:10.957131Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_dummies(dataset, variable_list):\n",
    "    for var in variable_list:\n",
    "        dataset = pd.concat([dataset, pd.get_dummies(dataset[var], prefix=var)], axis=1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:11.710141Z",
     "start_time": "2019-10-15T03:04:11.637647Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = gen_dummies(good_data, [\"primary_focus_area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-15T03:04:12.600507Z",
     "start_time": "2019-10-15T03:04:12.464814Z"
    }
   },
   "outputs": [],
   "source": [
    "good_data = gen_dummies(good_data, [\"resource_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list(good_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed=good_data[['time_fund_med',\n",
    "                   'len_text',\n",
    "                     'num_words',\n",
    "                     'posting_year',\n",
    "                     'posting_month',\n",
    "                     'totalPrice','numStudents',\n",
    "                     'primary_focus_area_Applied Learning',\n",
    "                     'primary_focus_area_Health & Sports',\n",
    "                     'primary_focus_area_History & Civics',\n",
    "                     'primary_focus_area_Literacy & Language',\n",
    "                     'primary_focus_area_Math & Science',\n",
    "                     'primary_focus_area_Music & The Arts',\n",
    "                     'primary_focus_area_Special Needs',\n",
    "                     'resource_type_Books',\n",
    "                     'resource_type_Other',\n",
    "                     'resource_type_Supplies',\n",
    "                     'resource_type_Technology',\n",
    "                     'resource_type_Trips',\n",
    "                     'resource_type_Visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed = trimmed.dropna()\n",
    "trimmed = trimmed.reset_index(drop=True)\n",
    "#trimmed = trimmed.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed['posting_year']=pd.to_numeric(trimmed['posting_year'])\n",
    "trimmed['totalPrice']=pd.to_numeric(trimmed['totalPrice'])\n",
    "trimmed = trimmed.astype({\"numStudents\": str})\n",
    "\n",
    "students=[]\n",
    "for ns in trimmed['numStudents']:\n",
    "    if ns is None:\n",
    "        x = 0\n",
    "    else:\n",
    "        try:\n",
    "            x = int(ns)\n",
    "        except:\n",
    "            x = 0\n",
    "    students.append(x)\n",
    "\n",
    "\n",
    "trimmed['numStudents']=pd.DataFrame(students)\n",
    "#trimmed['numStudents'].value_counts()\n",
    "#trimmed['numStudents']=pd.to_str(trimmed['numStudents'])\n",
    "#trimmed['numStudents'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary output = is a project funded or not?\n",
    "y = trimmed.time_fund_med\n",
    "\n",
    "#\n",
    "x = trimmed.drop(['time_fund_med'], axis=1)\n",
    "x_scaled = preprocessing.scale(x)\n",
    "# create training and testing vars\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_scaled, y, test_size=0.25)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a model\n",
    "logistic_regression = LogisticRegression()\n",
    "model = logistic_regression.fit(x_train, y_train)\n",
    "predictions = logistic_regression.predict(x_test)\n",
    "\n",
    "print(\"Score:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
