{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date #for time duration calculations\n",
    "from dateutil.parser import parse #for fuzzy finding year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #for saving output files, pickles\n",
    "from sys import stdout\n",
    "import time #for time.sleep function to delay calls\n",
    "from tqdm import tqdm #for updating loop\n",
    "#from os import listdir\n",
    "#from os.path import isfile, join\n",
    "import glob #pattern matching and expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "from sqlalchemy.sql import table, column, select, update, insert\n",
    "import psycopg2\n",
    "from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#In Python: Define your username and password used above. I've defined the database name (we're \n",
    "#using a dataset on births, so I call it birth_db). \n",
    "dbname = 'donors_db'\n",
    "username = 'russell'\n",
    "pswd = 'bradypodion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://russell:bradypodion@localhost/donors_db\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n"
     ]
    }
   ],
   "source": [
    "## 'engine' is a connection to a database\n",
    "## Here, we're using postgres, but sqlalchemy can connect to other things too.\n",
    "engine = create_engine('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print('postgresql://%s:%s@localhost/%s'%(username,pswd,dbname))\n",
    "print(engine.url)\n",
    "# Replace localhost with IP address if accessing a remote server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "postgresql://russell:bradypodion@localhost/donors_db\n"
     ]
    }
   ],
   "source": [
    "## create a database (if it doesn't exist)\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "print(engine.url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This block reads in archival data (from the web.archive.org) and old data from (an Insight Fellow proj on AWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data from: https://web.archive.org/web/20121019202946/http://developer.donorschoose.org/the-data\n",
    "#### Look at the first few rows of the CSV file\n",
    "arch_data = pd.read_csv(\"/home/russell/Documents/DataScience/DonorsChoose/Data/donorschoose-org-1may2011-v1-projects.csv\")\n",
    "\n",
    "######drop 'active' projects\n",
    "\n",
    "orig_rowlen=len(arch_data.index)\n",
    "\n",
    "arch_data = arch_data[arch_data.funding_status != 'live']\n",
    "\n",
    "new_rowlen=len(arch_data.index)\n",
    "\n",
    "print(\"***Web Archive Data\")\n",
    "\n",
    "print(\"original rows =\"+str(orig_rowlen)+\", new rows =\"+str(new_rowlen))\n",
    "\n",
    "#### data from https://github.com/adilmoujahid/DonorsChoose_Visualization/issues/10\n",
    "old_df = pd.read_csv(\"/home/russell/Downloads/opendata_projects.csv\", thousands = ',')\n",
    "\n",
    "######drop 'active' projects\n",
    "\n",
    "old_rowlen=len(old_df.index)\n",
    "\n",
    "old_df = old_df[old_df.funding_status != 'live']\n",
    "\n",
    "new_oldrowlen=len(old_df.index)\n",
    "\n",
    "print(\"****Github Data\")\n",
    "\n",
    "print(\"original rows =\"+str(old_rowlen)+\", new rows =\"+str(new_oldrowlen))\n",
    "\n",
    "#get shared column names, keeping order\n",
    "\n",
    "keepcolumns=set(old_df.columns).intersection(arch_data.columns)\n",
    "\n",
    "A=old_df.columns.values.tolist()\n",
    "\n",
    "keepcolumns=sorted(keepcolumns, key=A.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set(old_df.columns).difference(arch_data.columns) #columns not shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#############################\n",
    "## keep only shared columns, then combine\n",
    "\n",
    "arch_data = arch_data[keepcolumns]\n",
    "\n",
    "old_df = old_df[keepcolumns]\n",
    "\n",
    "bigold = arch_data.append(old_df) # combine\n",
    "\n",
    "bigold['calendar_completed']=bigold.date_completed.str.split(' ').str[0]\n",
    "\n",
    "bigold['year_completed']=bigold.calendar_completed.str.split('-').str[0]\n",
    "\n",
    "bigold['calendar_expired']=bigold.date_expiration.str.split(' ').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "del old_df;del arch_data\n",
    "\n",
    "print(bigold.shape)\n",
    "\n",
    "bigold.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigold['funding_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### insert data into database from Python (proof of concept - this won't be useful for big data, of course)\n",
    "\n",
    "bigold.to_sql('hist_projects', engine, if_exists='append',chunksize=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect:\n",
    "con = None\n",
    "\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "\n",
    "### query:\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM hist_projects WHERE school_state='NY';\n",
    "\"\"\"\n",
    "\n",
    "old_NYdata_from_sql = pd.read_sql_query(sql_query,con)\n",
    "\n",
    "old_NYdata_from_sql.head(2)\n",
    "\n",
    "### Close communication with the database\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149420, 47)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect:\n",
    "con = None\n",
    "con = psycopg2.connect(database = dbname, user = username, host='localhost', password=pswd)\n",
    "\n",
    "# Close communication with the database\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#get all pickle files from folder into a list, 'picks'\n",
    "\n",
    "picks = sorted(glob.glob(\"/home/russell/Documents/DataScience/DonorsChoose/Data/Metrics/*pickle\")) \n",
    "\n",
    "print(len(picks))\n",
    "\n",
    "\n",
    "#this will create a master list with the same # of elements as projects\n",
    "basic_list = []\n",
    "\n",
    "for pf in picks:\n",
    "    #https://stackoverflow.com/a/3249684/1602288\n",
    "    stdout.write(\"\\r%s\" % pf)\n",
    "    stdout.flush()\n",
    "    check=pickle.load(open(pf,\"rb\"))\n",
    "    #check=check[['id','proposalURL']]\n",
    "    if (isinstance(check, pd.DataFrame)) and (len(check.index)>0):\n",
    "        basic_list.append(check)\n",
    "    #basic_list.append(check)\n",
    "    \n",
    "    \n",
    "    #basic_list.append(pickle.load(open(pf,\"rb\")))\n",
    "    #basic_list = basic_list+(pickle.load(open(picks[0],\"rb\")))\n",
    "    #sleep(.4)\n",
    "stdout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1642201, 37)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigframe = pd.concat(basic_list)\n",
    "#schoolTypes, teacherTypes #these variables are 'dictionaries' and need to be dealt with\n",
    "bigframe=bigframe.drop(['schoolTypes', 'teacherTypes'], axis=1)\n",
    "bigframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## insert data into database from Python\n",
    "\n",
    "#### bigframe.to_sql('scraped_project_metrics', engine, if_exists='append',chunksize=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic_list =pickle.load(open(\"/home/russell/Documents/DataScience/DonorsChoose/Data/BigFrame.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check=\"/home/russell/Documents/DataScience/DonorsChoose/Data/Biglist\"\n",
    "pickle_out = open(check+'.pickle',\"wb\")\n",
    "pickle.dump(basic_list, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nybig = bigframe[bigframe.state.eq('NY')]\n",
    "nybig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/40121869/1602288\n",
    "ffd = nybig['fullyFundedDate'].values.tolist()\n",
    "\n",
    "years=[]\n",
    "\n",
    "for date in ffd:\n",
    "    try:\n",
    "        years.append(parse(str(date), fuzzy=True).year)\n",
    "    except:   \n",
    "        years.append(np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nybig['ffyear']=years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nybig.shape)\n",
    "print(nydf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nybig: 'latitude','longitude','ffyear'\n",
    "#nydf: 'school_latitude', 'school_longitude','year_completed'\n",
    "\n",
    "nybig['latitude']=nybig.latitude.astype(float)\n",
    "nybig['longitude']=nybig.longitude.astype(float)\n",
    "nybig = nybig.fillna(0)\n",
    "nybig['ffyear']=nybig.ffyear.astype(int)\n",
    "nybig['numDonors']=nybig.numDonors.astype(int)\n",
    "\n",
    "nydf['school_latitude'] = nydf.school_latitude.astype(float)\n",
    "nydf['school_longitude'] =nydf.school_longitude.astype(float)\n",
    "nydf = nydf.fillna(0)\n",
    "nydf['year_completed']=nydf.year_completed.astype(int)\n",
    "nydf['num_donors']=nydf.num_donors.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nybig['expirationDate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nydf['calendar_expired']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/a/41815118/1602288\n",
    "new_ny = pd.merge(nybig,nydf,left_on=['latitude','longitude','ffyear','numDonors','expirationDate'],right_on = ['school_latitude', 'school_longitude','year_completed','num_donors','calendar_expired'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ny.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(new_ny.loc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigframe['expirationTime'] = bigframe['expirationTime'].apply(str)\n",
    "bigframe['expirationTime']=bigframe['expirationTime'].str[1:-5]\n",
    "bigframe['timeback']=pd.to_numeric(bigframe['expirationTime'])\n",
    "bigframe['right_date']=pd.to_datetime(bigframe['expirationDate'],format='%Y-%m-%d')\n",
    "bigframe['start_date'] = bigframe.apply(lambda row: row['right_date'] - timedelta(seconds=row['timeback']),axis = 1)\n",
    "\n",
    "#bigframe['right_date'] - timedelta(seconds=bigframe['timeback'])\n",
    "\n",
    "\n",
    "\n",
    "bigframe.head(3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigframe['right_date']=pd.to_datetime(bigframe['expirationDate'],format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bigframe['right_date']=pd.to_datetime(bigframe['expirationDate'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_date = bigframe.expirationDate.values[0]\n",
    "type(exp_date)\n",
    "#right_date = date.fromisoformat(exp_date) #gets date into a datetime.date format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to back calculate 'start time' using *expirationDate* & *expirationTime*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fullyFundedDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(bigframe['proposalURL'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(bigframe['expirationTime'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for colu in bigframe.columns:\n",
    "    print(bigframe[colu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 999)\n",
    "bigframe.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line (to_sql) is doing a lot of heavy lifting. It's reading a dataframe, it's creating a table, and adding the data to the table. So ** SQLAlchemy is quite useful! **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigframe.shape)\n",
    "print(type(bigframe))\n",
    "print(bigframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ok = bigframe.loc[bigframe.astype(str).drop_duplicates().index]\n",
    "ok = bigframe.drop_duplicates(subset='id') #drop duplicate rows\n",
    "print(type(ok))\n",
    "print(ok.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = ok.drop_duplicates(subset=\"id\", keep=\"first\") #for some reason, some duplicate projs were kept, this drops'em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "#d.set_option(\"max_rows\", None) #undo by resetting --- pd.reset_option(“max_rows”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trailerdict = dict(zip(ok['id'],ok['fulfillmentTrailer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open('/home/russell/Documents/DataScience/DonorsChoose/Data/trailers.pickle',\"wb\")\n",
    "pickle.dump(trailerdict, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donordict = dict(zip(ok['id'],ok['numDonors']))\n",
    "pickle_out = open('/home/russell/Documents/DataScience/DonorsChoose/Data/donor_num.pickle',\"wb\")\n",
    "pickle.dump(donordict, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trailers =ok['fulfillmentTrailer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"poster\", font_scale=1.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "sns.distplot(ok[\"numDonors\"].dropna())\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bigframe['fulfillmentTrailer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigframe['id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_trailers = bigframe['fulfillmentTrailer'].array\n",
    "print(f_trailers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a = what.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(10):\n",
    "    #print(b)\n",
    "    print(bigframe['fulfillmentTrailer'].array[b])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigframe['proposalURL'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkdf = pd.DataFrame(project_IDs,columns =['proj_id'])\n",
    "checkdf\n",
    "pd.set_option(\"max_rows\", None) #undo by resetting --- pd.reset_option(“max_rows”)\n",
    "checkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkdf['proj_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bewild = pd.DataFrame.from_dict(lookat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "#modified scrapy settings here:\n",
    "#/home/russell/anaconda3/envs/insight/lib/python3.8/site-packages/scrapy/settings\n",
    "#to include the user agents described here: https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/russell/Downloads/opendata_projects.csv\", thousands = ',')\n",
    "pd.set_option('display.max_columns', None)\n",
    "#pd.set_option(\"max_rows\", None) #undo by resetting --- pd.reset_option(“max_rows”)bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current = \"https://www.donorschoose.org/common/json_feed.html?showFacetCounts=true&APIKey=H9v7hCeN&max=100&index=0\"\n",
    "historical = \"https://www.donorschoose.org/common/json_feed.html?showFacetCounts=true&APIKey=H9v7hCeN&max=40&historical=true&index=0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(historical)\n",
    "data_dict = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_dict.items():\n",
    "    print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_on_this_page = list(data_dict.values())\n",
    "\n",
    "proposal_ind = ([list(data_dict.keys()).index('proposals')])[0]  #\n",
    "\n",
    "proposal_list = dicts_on_this_page[proposal_ind]\n",
    "#dicts_on_this_page[proposal_ind]\n",
    "#proposal_ind = which(data.keys()=='proposals')\n",
    "#print(proposal_ind)\n",
    "#print(proposals_on_this_page[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_proposal = proposal_list[0] #returns dictionary of first proposal items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(first_proposal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in first_proposal.items():\n",
    "    print (key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _DRAW OUT SCREENS and FUNCTIONALITY GOALS FOR END OF WEEK_\n",
    "\n",
    "\n",
    "\n",
    "## MONDAY\n",
    "    Well scoped, clearly-defined problem + some data\n",
    "## \tTUESDAY\n",
    "    SQL[organized data] \n",
    "## \tWED \n",
    "    analytics/working algo/some results ----> (python linked to sql data)\n",
    "## \tTHUR – SQL mapped to PYTHON connected to FLASK (or something)\n",
    "\n",
    "\n",
    "# GOOD Qs to ASK SELF AND OTHERS\n",
    "#### \t-What’s actionable about your product?\n",
    "#### \t-Did you try other models?\n",
    "#### \t-Is this better than random?\n",
    "#### \t-Is this better than the simplest model?\n",
    "#### \t-Why did you choose these inputs?\n",
    "#### \t-How did you validate this?\n",
    "#### \t-What are your metrics for success?\n",
    "#### \t-What are the assumptions of your model?\n",
    "#### \t-How would you improve this project with more time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['primary_focus_area'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expired = df[(df['funding_status']=='expired')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['_teacher_acctid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
